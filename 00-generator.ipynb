{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by using the [markovify](https://github.com/jsvine/markovify/) library to make some social-media-sized utterances in the style of Jane Austen.  This will be the basis for generating a synthetic social media stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No!\n",
      "In spite of this amendment, however, she requested to have a nearer view of their characters and her own spirits, were at least very certain; and she joined Mrs. Jennings in the drawing-room for me.\n",
      "When alone with Elizabeth afterwards, she spoke more on the subject; and her father was universally civil, but not one with such justice.\n",
      "After being nursed up at Mansfield, it was too late to be saving.\n",
      "The journey was likely to be good.\n",
      "The work of one moment was destroyed by the next.\n",
      "Catherine's spirits revived as they drove up to the yard-arm.\n",
      "She had a sister married to a Miss Hawkins.\n",
      "It might not be thought of: he was very much obliged to her for the world.\n",
      "I was never so enraged before, and must relieve myself by writing to you, for I do not think any young woman who had listened and seemed to think it as much her duty as Mrs. Weston's to receive them.\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "import codecs\n",
    "\n",
    "with codecs.open(\"data/austen.txt\", \"r\", \"cp1252\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "austen_model = markovify.Text(text, retain_original=False, state_size=3)\n",
    "\n",
    "for i in range(10):\n",
    "    print(austen_model.make_short_sentence(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use spaCy to identify entities (mostly proper nouns and noun phrases) in these synthetic status updates and turn them into hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her congratulations were warm and open; but Emma could not but dispose them all to go back into Norfolk directly, and put everything at once on such a point, without being resented.\n",
      "Her congratulations were warm and open; but #Emma could not but dispose them all to go back into #Norfolk directly, and put everything at once on such a point, without being resented.\n",
      "Charming people, Mr. and Mrs. Perry.\n",
      "Charming people, Mr. and Mrs. #Perry.\n",
      "Fanny looked very angry too, and her husband away, she can have nothing to say or to hear that Mrs. Ferrars considered it in that light; a very gratifying circumstance you know to us all.\n",
      "#Fanny looked very angry too, and her husband away, she can have nothing to say or to hear that Mrs. #Ferrars considered it in that light; a very gratifying circumstance you know to us all.\n",
      "He stopped in his earnestness to look the question, and the gentlemen did not disdain.\n",
      "He stopped in his earnestness to look the question, and the gentlemen did not disdain.\n",
      "To be finding herself, perhaps within three days, transported to Mansfield, was an image of the greatest assistance to her; and the tranquillity of Catherine.\n",
      "To be finding herself, perhaps within #threedays, transported to #Mansfield, was an image of the greatest assistance to her; and the tranquillity of #Catherine.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def make_sentence(model, length=200):\n",
    "    return model.make_short_sentence(length)\n",
    "    \n",
    "def hashtagify_full(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        sentence = sentence.replace(str(ent), \"#%s\" % str(ent).replace(\" \", \"\"))\n",
    "    return (sentence, [\"#%s\" % str(ent).replace(\" \", \"\") for ent in doc.ents])\n",
    "\n",
    "def hashtagify(sentence):\n",
    "    result,_ = hashtagify_full(sentence)\n",
    "    return result\n",
    "\n",
    "for i in range(5):\n",
    "    sentence = make_sentence(austen_model)\n",
    "    print(sentence)\n",
    "    print(hashtagify(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now train two Markov models on positive and negative product reviews (taken from the [public-domain Amazon fine foods reviews dataset on Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews/)).  We'll incorporate the results of these models into our synthetic social media stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def train_markov_gz(fn):\n",
    "    \"\"\" trains a Markov model on gzipped text data \"\"\"\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return markovify.Text(text, retain_original=False, state_size=3)\n",
    "\n",
    "negative_model = train_markov_gz(\"data/reviews-1.txt.gz\")\n",
    "positive_model = train_markov_gz(\"data/reviews-5-100k.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most movie theaters have finally changed from coconut oil to canola or other good oils to address this major health concern and bad press over the years.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sentence(negative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The garlic flavor is pervasive throughout, and it is a perfect size; you get a fresh product.While they could do for me.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sentence(positive_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these models with relative weights, but this yields somewhat unusual results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_model = markovify.combine([austen_model, negative_model, positive_model], [14, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rather it's a much healthier bone or treat than most stuff on the planet.\n",
      "But it is done; we are reconciled, dearer, much dearer, than ever, and with eyes closed, as if the beans are in #BPA free plastic.\n",
      "She drinks tea #everynight for #10years ! It is the species #Mentha arvensis.\n",
      "No complaints.\n",
      "He engaged to be with the #Musgroves from breakfast to dinner.\n",
      "Other teas are bitter in comparison.Try it you'll like it.\n",
      "Perfect for quashing that sweet craving without breaking my extremely low calorie diet.\n",
      "The package I received #today.\n",
      "There's no way this was random either.\n",
      "We found it so difficult to chew.#Described as the perfect gift and I will go back to Raw Revolution.This bar is disgusting ...\n",
      "I hadn't made jam in over #tenyears, so I wasn't sure what the other reviewers are thinking when they created this variety box if you buy this your nuts.\n",
      "The only reason I gave this a try..This does NOT taste anything like these beverages and you don't want the liability of these items in the order so that I don't run out!\n",
      "If you like escargo then this is a great value when I buy this coffee for #every6minutes of my husband #over65yearsold and has reflux and a protein snack is just the right size.\n",
      "I'm just a nut person.\n",
      "When I replied, asking for better info = no response.\n",
      "We actually like these better than #BettyCrocker, which are kind of hard.\n",
      "This is not the #Snow's clam chowder I grew up enjoying #Rice A #Roni and it's vermicelli/rice mix.\n",
      "Thankfully, he was only the more basic claims have been substantiated.\n",
      "This jerky is; tough, fat, has gristle, and does not have the grief of seeing _you_ unable to respect your partner in life.\n",
      "It's amazing.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(hashtagify(make_sentence(compound_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is more or less the case in the real world, we'll assume that a small percentage of users are responsible for the bulk of social media activity, and that the bulk of users are responsible for relatively few posts.  We'll model this with a table of random user IDs that has a collection of relatively few talkative users and relatively many moderate users; the proportion of utterances from talkative users to utterances from moderate users is the inverse of the proportion of talkative users to moderate users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "import collections\n",
    "\n",
    "class UserTable(object):\n",
    "    def __init__(self, size, weights=[8, 2]):\n",
    "        self._talkative = collections.deque()\n",
    "        self._moderate = collections.deque()\n",
    "        self._size = size\n",
    "        self._cutoff = float(weights[0]) / sum(weights)\n",
    "        \n",
    "        for i in range(size):\n",
    "            new_uid = math.floor(numpy.random.uniform(10 ** 10))\n",
    "            if numpy.random.uniform() >= self._cutoff:\n",
    "                self._moderate.append(new_uid)\n",
    "            else:\n",
    "                self._talkative.append(new_uid)\n",
    "    \n",
    "    def random_uid(self):\n",
    "        def choose_from(c):\n",
    "            return c[math.floor(numpy.random.uniform() * len(c))]\n",
    "        \n",
    "        if numpy.random.uniform() >= self._cutoff:\n",
    "            return choose_from(self._talkative)\n",
    "        else:\n",
    "            return choose_from(self._moderate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the number of times each user ID appears if we ask the `UserTable` for 1000 random user IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15a72bac8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ut = UserTable(100)\n",
    "uids = [ut.random_uid() for i in range(1000)]\n",
    "seaborn.countplot(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweets(models, weights=None, hashtag_weights=[8, 2], ut=None, seed_hashtags=[]):\n",
    "    if weights is None:\n",
    "        weights = [1] * len(models)\n",
    "    \n",
    "    if ut is None:\n",
    "        ut = UserTable(10000)\n",
    "    \n",
    "    choices = []\n",
    "    \n",
    "    total_weight = float(sum(weights))\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        choices.append((float(sum(weights[0:i+1])) / total_weight, models[i]))\n",
    "    \n",
    "    def choose_model():\n",
    "        r = numpy.random.uniform()\n",
    "        for (p, m) in choices:\n",
    "            if r <= p:\n",
    "                return m\n",
    "        return choices[-1][1]\n",
    "    \n",
    "    seen_hashtags = set()\n",
    "    hashtags = []\n",
    "    total_hashtag_weight = float(sum(hashtag_weights))\n",
    "    for i in range(len(hashtag_weights)): \n",
    "        hashtags.append((float(sum(hashtag_weights[0:i+1])) / total_hashtag_weight, collections.deque()))\n",
    "    \n",
    "    iws = [1.0 - w for (w, _) in hashtags]\n",
    "    inverse_weights = [(sum(iws[0:i+1]), i) for _, i in zip(iws, range(len(iws)))]    \n",
    "\n",
    "    def choose_from(c):\n",
    "        idx = math.floor(numpy.random.uniform() * len(c))\n",
    "        return c[idx]\n",
    "    \n",
    "    def store_hashtag(tag):\n",
    "        if tag not in seen_hashtags:\n",
    "            seen_hashtags.add(str(tag))\n",
    "            r = numpy.random.uniform()\n",
    "            for(p, deq) in hashtags:\n",
    "                if r <= p:\n",
    "                    deq.append(tag)\n",
    "    \n",
    "    def choose_hashtag():\n",
    "        r = numpy.random.uniform()\n",
    "        for(p, i) in hashtags:\n",
    "            if r <= - p and len(hashtags[i][1]) > 0:\n",
    "                return choose_from(hashtags[i][1])\n",
    "        return len(hashtags[0][1]) > 0 and choose_from(hashtags[0][1]) or choose_from(hashtags[1][1])\n",
    "    \n",
    "    for tag in seed_hashtags:\n",
    "        seen_hashtags.add(str(tag))\n",
    "        hashtags[-1][1].append(str(tag))\n",
    "    \n",
    "    while True:\n",
    "        tweet, tags = hashtagify_full(make_sentence(choose_model()))\n",
    "        for tag in tags:\n",
    "            store_hashtag(str(tag))\n",
    "        \n",
    "        this_tweet_tags = set([str(t) for t in tags])\n",
    "        \n",
    "        if len(seen_hashtags) > 0:\n",
    "            for i in range(min(numpy.random.poisson(3), len(seen_hashtags))):\n",
    "                tag = choose_hashtag()\n",
    "                if str(tag) not in this_tweet_tags:\n",
    "                    this_tweet_tags.add(str(tag))\n",
    "                    tweet += \" %s\" % str(tag)\n",
    "            \n",
    "        yield (ut.random_uid(), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_hashtags=[\"#ff\", \"#marketing\", \"#fail\", \"#followfriday\", \"#yolo\", \"#retweet\", \"#tbt\", \"#socialmedia\", \"#startup\", \"#blogpost\", \"#news\", \"#health\"]\n",
    "\n",
    "t = generate_tweets([austen_model, positive_model, negative_model, compound_model], [22, 4, 4, 2], seed_hashtags=seed_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object generate_tweets at 0x15a724fc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7350786372,\n",
       "  'I was delighted to find these 8-pack bundles available on #Amazon at such a good product.'),\n",
       " (6597221724, 'It will be well pleased. #Amazon'),\n",
       " (1166998778,\n",
       "  'Emma hung about him affectionately, and smiled, and hoped her turn was coming soon. #Amazon'),\n",
       " (8815084594,\n",
       "  'A long grain, but it is not the abswer to your problems. #Amazon'),\n",
       " (8660921049,\n",
       "  'Everybody was surprised, and #Darcy, after looking at her mother, she is saving enough. #Amazon'),\n",
       " (1283837519,\n",
       "  '#DIAMOND MAKES A GOOD PRODUCT AND THESE #WALNUTS ARE DELICIOUS ON THEIR OWN AS A SNACK AND SHE LOVES THEM. #Darcy'),\n",
       " (6392744288,\n",
       "  'I am myself partial to the house together, there to lounge away the time as they could after dinner; and every one concerned was looking forward with eagerness. #DIAMOND #Darcy'),\n",
       " (4461427418, 'My cat loves these. #Darcy #DIAMOND #Amazon'),\n",
       " (4125085827,\n",
       "  'The most trivial, paltry, insignificant part; the merest commonplace; not a tolerable speech in the whole. #DIAMOND'),\n",
       " (6314882860,\n",
       "  'He was only too agreeable for #Catherine to be exactly what he could for the children of so unsuitable a match. #Amazon #DIAMOND'),\n",
       " (2241037370,\n",
       "  'Every body seemed happy; and the praise of our dear little Neice the innocent #Louisa, who is at present the garden. #Amazon #Catherine'),\n",
       " (1897866325,\n",
       "  'At the further end of the house is dreadful. #Louisa #Catherine #Amazon'),\n",
       " (1467420207,\n",
       "  'It makes you look like an old witch. #Amazon #Catherine #Darcy'),\n",
       " (619112629, 'If I had not known for #manyhours. #Louisa #Amazon #Catherine'),\n",
       " (2607698672,\n",
       "  'And to my surprise, she told me when she would lay in her bed on her back teeth. #Amazon #Louisa'),\n",
       " (2013635605,\n",
       "  'As Mr #Shepherd perceived that this connexion of the Crofts did them no service with Sir #Walter, he may not forget her. #Louisa #Catherine'),\n",
       " (8373606380,\n",
       "  \"I'm not one for moderation when it comes time to eat. #DIAMOND\"),\n",
       " (7701170063,\n",
       "  '_You_ must set the example. #DIAMOND #Walter #Louisa #Catherine #Amazon #Darcy'),\n",
       " (5029018423,\n",
       "  'You have to keep them interested.Good price for such an inexpensive price!! such a great product and i must tell you that the lid came off when the weather heats up this #summer! #Louisa #Shepherd'),\n",
       " (5890058023, 'The issue of all depended on #one question. #summer'),\n",
       " (2955807339,\n",
       "  'I should have ordered instant #NongShim noodles instead. #Shepherd #Darcy #Catherine'),\n",
       " (9981611390,\n",
       "  'And they will now see their cousin treated as she ought to be respected. #Shepherd #Amazon'),\n",
       " (2420132230,\n",
       "  'I have disconcerted him already by my calm reserve, and it shall be done. #Amazon #Shepherd #summer'),\n",
       " (9078779063,\n",
       "  \"They're not too sweet, which #JerkSeasoning is not. #DIAMOND #summer #Louisa\"),\n",
       " (703362260, 'You may believe me, Mrs. #Weston. #Amazon'),\n",
       " (2035822658,\n",
       "  '#Anne, finding she might decline it, did so, very gratefully. #Louisa #Weston #Darcy'),\n",
       " (2371211921, 'Do not defer it. #Weston #Amazon #DIAMOND'),\n",
       " (4853014873,\n",
       "  'Of the rest she saw nothing: nobody seemed to think of you. #DIAMOND #Walter #JerkSeasoning'),\n",
       " (7793193718,\n",
       "  'But after reading the reviews here, the price, and we eat it at once, she will hide it for later use. #DIAMOND #Weston #summer #NongShim'),\n",
       " (1628510574, 'What could be simpler? #Catherine #NongShim'),\n",
       " (8203681254,\n",
       "  'He came; and he entered the house, and with proportionate speed through the neighbourhood. #Walter #Amazon #JerkSeasoning #Louisa'),\n",
       " (2920836487,\n",
       "  '#Behold him there, the monarch of the seas! #JerkSeasoning #summer'),\n",
       " (4294780633, 'It is a very good estate. #Darcy #Shepherd'),\n",
       " (6124110349,\n",
       "  'Do you think it is a time for work. #JerkSeasoning #Weston #Walter'),\n",
       " (2294336147, 'I tried it. #DIAMOND #JerkSeasoning #Darcy'),\n",
       " (3105528706,\n",
       "  'Make haste, make haste. #Catherine #NongShim #summer #JerkSeasoning'),\n",
       " (7303503394,\n",
       "  'You are more than Mortal. #Catherine #Walter #summer #Darcy #Louisa #NongShim'),\n",
       " (2734452901,\n",
       "  'I ended up with one of the old formula that was in my mouth. #Louisa #Amazon #summer'),\n",
       " (6877834554,\n",
       "  'The bar itself seemed extremely stale to me. #Catherine #Darcy'),\n",
       " (7800010086, 'I am undismayed however. #Catherine #Walter'),\n",
       " (1504909127,\n",
       "  '#Elizabeth joined them again only to say in return; but then, you would have no motive for writing strong enough to pay. #Shepherd #NongShim'),\n",
       " (1062250422,\n",
       "  'Mary deplored the necessity for herself. #summer #Shepherd #DIAMOND'),\n",
       " (757592579,\n",
       "  'I must come to an understanding with him. #JerkSeasoning #Louisa'),\n",
       " (8759331851,\n",
       "  \"It inspired little more than hints of what might be passing in #BerkeleyStreet during their absence; but a moment's glance at her companion to observe its effect on her. #summer #Catherine #JerkSeasoning #Darcy\"),\n",
       " (7184324209,\n",
       "  'Now #Mary, I declare it was so, by that which only could convince her, a better fate than to be chosen, to excite gratitude than to feel it. #Shepherd #Catherine #Walter #Weston'),\n",
       " (336311719,\n",
       "  'Is #Allenham the only house in #theevening. #Louisa #JerkSeasoning #Mary #NongShim #Darcy'),\n",
       " (7803819769,\n",
       "  'I went though for #weeks on end.I was also feeling so guilty because I gave it to my Lab who will eat almost anything, found the noodles unpleasant to eat and relieve pain from teething, too. #DIAMOND #Mary #Walter'),\n",
       " (781369271,\n",
       "  'My #8montholds are not pickey eaters but they refused to contact me. #summer'),\n",
       " (7942348004, 'For #halfanhour a day, without a fire. #weeks #Mary #Amazon'),\n",
       " (7489702665,\n",
       "  \"Hopefully they'll reassess the seasoning on #alittlelatenight snack because this one is way to bland and the flavor non-existent. #Shepherd #Weston\"),\n",
       " (1662134237,\n",
       "  'There she was welcomed, with the utmost delight, by her father, who perfectly approves the acquaintance, should put an end to what might be.'),\n",
       " (3745660659,\n",
       "  'But I am pleased that #StewartCandy took the time to do their research before promoting these products. #Catherine #Shepherd #alittlelatenight'),\n",
       " (4440174629,\n",
       "  'The enclosure of #NorlandCommon, now carrying on, is a most insolent thing, indeed, and I wonder Mrs. #Thorpe should allow it. #Catherine #BerkeleyStreet #JerkSeasoning'),\n",
       " (2332078012, 'It implies everything amiable. #Weston #halfanhour'),\n",
       " (6176993736,\n",
       "  '#Catherine, interested at once by ever possible attention to her while she draws, that in fact he knows nothing of the matter. #Allenham #Darcy #StewartCandy'),\n",
       " (7832710793,\n",
       "  'The Mr Musgroves had their own pursuits, the ladies proceeded on their own business, and they met no more while #Anne belonged to them. #Allenham #8montholds #NongShim #weeks'),\n",
       " (9274267713, 'Looks great with any meal. #Shepherd #8montholds #weeks'),\n",
       " (6968733139,\n",
       "  'Nobody is healthy in #London, nobody can be. #Shepherd #JerkSeasoning'),\n",
       " (6402290317,\n",
       "  \"In revolving #LadyCatherine's expressions, however, she could not help watching all that passed. #StewartCandy #8montholds #Allenham #London #Louisa\"),\n",
       " (7054388297,\n",
       "  'Your package will be so happy if they lower this to #around$1.00 or less per bag. #BerkeleyStreet #Amazon #Walter #StewartCandy'),\n",
       " (2033748793,\n",
       "  'He was, perhaps, but at treacherous play with her. #8montholds #Weston #Thorpe'),\n",
       " (9663029189,\n",
       "  'SHE HAS #HAD FOR OVER #AYEAR AND PREFERS TO ALL TREATS AND FOOD. #BerkeleyStreet #NongShim #alittlelatenight #London'),\n",
       " (7098100679,\n",
       "  'No, no, you will proceed into this small vaulted room, and through this into several others, without perceiving anything very remarkable in either. #Thorpe #Catherine'),\n",
       " (5953707103,\n",
       "  'The grounds were declared to be the right, she immediately found herself in the bliss of the moment. #summer'),\n",
       " (4828762253,\n",
       "  \"Now I don't want to take that chance with your beloved pooch! #HAD #JerkSeasoning #Allenham #halfanhour\"),\n",
       " (8544761046,\n",
       "  'But something must be the man; and no civility shall be wanting on my part that might inconvenience either herself or her coachman. #NongShim #Darcy #BerkeleyStreet'),\n",
       " (7549810815,\n",
       "  'It is creamy and more natural if #one prefers only subtle richness.This tea is absolutely delicious. #summer #Louisa #StewartCandy'),\n",
       " (3058986484,\n",
       "  'I may have my daughter appropriately examined. #Weston #BerkeleyStreet #StewartCandy'),\n",
       " (5758790083,\n",
       "  'This product is a great product. #Shepherd #Amazon #DIAMOND #NongShim'),\n",
       " (8016231964,\n",
       "  'And besides that, my cousin #Richard said himself, that when it came to be paid; and then there was such a counterpoise of good as might console her for the present a cripple. #Shepherd #London #around$1.00'),\n",
       " (5975298289,\n",
       "  'Still, however, and during the length of the service, however, I admit to be sometimes too hard a stretch upon the mind. #JerkSeasoning #Amazon #Walter #halfanhour'),\n",
       " (3121262336,\n",
       "  'If you compare the various #Science diet formulae that have different health claims on them - the nutritional values are all that great, either. #Weston #Thorpe'),\n",
       " (3061534298,\n",
       "  'Such a companion for herself in the carriage to be in any danger from the deception never entered my head. #JerkSeasoning #HAD'),\n",
       " (7679074800,\n",
       "  'I dined twice in #WimpoleStreet, and might have been glad to change feelings with #Harriet, very glad to see him suffering, to know him to be guilty of. #BerkeleyStreet #halfanhour #Richard #Science #Mary'),\n",
       " (5058332533,\n",
       "  \"How is such a girl to be my own fault--because I will not be prevented, however, from even looking his surprise by his father's entrance. #Catherine #London\"),\n",
       " (6232177778,\n",
       "  \"#Anne had always felt that #Charlotte's opinion of matrimony was not exactly what she had lost, till it was all confusion. #WimpoleStreet #Amazon #Darcy\"),\n",
       " (5086492787,\n",
       "  \"I'm so sad because I would not recommend this product and was a NIGHTMARE and i had no idea who had sent the wrong kind... #Thorpe #JerkSeasoning\"),\n",
       " (4126690570,\n",
       "  'He had just dismounted;--she could not be offered to anybody else. #Science #AYEAR #Thorpe'),\n",
       " (1238847755,\n",
       "  'We shall live within #afewdays longer under his roof. #Walter #Darcy'),\n",
       " (9750820850,\n",
       "  'In my #Petsmart, #BlueBuffalo has done nothing to correct it. #Darcy #StewartCandy'),\n",
       " (7508845689,\n",
       "  'There was much to be gone. #8montholds #Darcy #Walter #JerkSeasoning #summer'),\n",
       " (9265284095,\n",
       "  'This was just such a summary view of the whole, which had exceedingly pleased him. #HAD #Louisa #NongShim #JerkSeasoning'),\n",
       " (3995335221,\n",
       "  '#One or #two meetings of this kind was done away. #afewdays #summer #Thorpe #DIAMOND'),\n",
       " (1118439093,\n",
       "  'WOW, interesting taste, not bitter at all, even when you leave the product in and #Amazon has the best price! #alittlelatenight #Walter'),\n",
       " (2757251756, 'Yum! #NongShim #JerkSeasoning #weeks'),\n",
       " (8363858317, \"You don't have to squint. #AYEAR #NongShim #around$1.00\"),\n",
       " (3998924788, 'I highly recommend it. #Walter #AYEAR'),\n",
       " (685368706,\n",
       "  \"To retire to bed, however, unsatisfied on such a heart as his; and could #Frederica's artless affection detach him from her heart, for he stopped to hand her out. #afewdays #Science\"),\n",
       " (5892839650,\n",
       "  'My #first case went very quickly, as every co-worker who tried them wanted a can of white shoe peg corn, and a package of the chili and garlic right out front. #HAD #DIAMOND'),\n",
       " (9166678928,\n",
       "  \"This pasta tastes and doesn't eat the same flavors were served back to back, gobble the same flavor characteristics of other #Assam teas - it is WONDERFUL! #Thorpe\"),\n",
       " (1842872647,\n",
       "  'My father means to fetch you himself, but it will be a full ball or not! #Shepherd #AYEAR #Thorpe'),\n",
       " (3579111970,\n",
       "  'As for Colonel #Brandon, she was not without the expectation of the others not to smile. #summer #BlueBuffalo #Richard'),\n",
       " (9642942896,\n",
       "  'She was almost sure it is, for I saw him you know for the #first time. #WimpoleStreet #two #Science'),\n",
       " (5186717172, 'Nothing wrong in him escaped her. #Weston #two #Louisa'),\n",
       " (2364511178,\n",
       "  'When I got this stuff and am not buying cat food as well, so of course there is no need for sugar at all. #Louisa #One #Darcy #AYEAR #DIAMOND #NongShim #BerkeleyStreet'),\n",
       " (5125240671,\n",
       "  'At #Churchhill, however, I must be satisfied about #MissPrice. #NongShim #StewartCandy #Richard #Mary'),\n",
       " (4135271104,\n",
       "  'It is instant coffee as others have described. #summer #Thorpe'),\n",
       " (9597394733,\n",
       "  'I was only pureeing poached pears and zucchini after all, nothing hard. #halfanhour #Amazon #Darcy #Assam'),\n",
       " (4849083837,\n",
       "  \"Mr. #Tilney drank tea with Mrs. and #MissBates, and to have #MissCrawford's liveliness repeated to her at the concert.\"),\n",
       " (201904180,\n",
       "  \"Always a fan of both, but this one is comparable and a great way to get extra B's...and if my kids will drink it... and they love them too. #Shepherd #afewdays #MissPrice #StewartCandy #DIAMOND\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[next(t) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "def timing(c):\n",
    "    for _ in range(c):\n",
    "        next(t)\n",
    "\n",
    "cProfile.run('timing(2000)', 'generatestats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 18 19:34:03 2019    generatestats\n",
      "\n",
      "         8231432 function calls (8057651 primitive calls) in 23.246 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "3998/2000    0.068    0.000   21.346    0.011 <ipython-input-11-22f0f6476ec7>:1(generate_tweets)\n",
      "     2000    0.003    0.000    0.006    0.000 <ipython-input-11-22f0f6476ec7>:15(choose_model)\n",
      "     5939    0.012    0.000    0.026    0.000 <ipython-input-11-22f0f6476ec7>:31(choose_from)\n",
      "     1229    0.004    0.000    0.009    0.000 <ipython-input-11-22f0f6476ec7>:35(store_hashtag)\n",
      "     5939    0.015    0.000    0.054    0.000 <ipython-input-11-22f0f6476ec7>:43(choose_hashtag)\n",
      "     2000    0.001    0.000    0.001    0.000 <ipython-input-11-22f0f6476ec7>:59(<listcomp>)\n",
      "      2/1    0.000    0.000    0.009    0.009 <ipython-input-15-801ca619797e>:3(timing)\n",
      "     2000    0.007    0.000    0.010    0.000 <ipython-input-2-ff5f1bd7c840>:11(<listcomp>)\n",
      "     2000    0.005    0.000    1.880    0.001 <ipython-input-2-ff5f1bd7c840>:4(make_sentence)\n",
      "     2000    0.044    0.000   14.390    0.007 <ipython-input-2-ff5f1bd7c840>:7(hashtagify_full)\n",
      "     2000    0.006    0.000    0.018    0.000 <ipython-input-8-1b5c0e8bc0df>:19(random_uid)\n",
      "     2000    0.006    0.000    0.009    0.000 <ipython-input-8-1b5c0e8bc0df>:20(choose_from)\n",
      "      2/1    0.000    0.000    0.009    0.009 <string>:1(<module>)\n",
      "     3140    0.001    0.000    0.001    0.000 __init__.py:19(_return_en)\n",
      "     2000    0.001    0.000    0.022    0.000 _methods.py:25(_amax)\n",
      "    26000    0.014    0.000    0.430    0.000 _methods.py:31(_sum)\n",
      "   220000    0.107    0.000    1.855    0.000 _methods.py:34(_prod)\n",
      "    60000    0.094    0.000    0.122    0.000 _methods.py:43(_count_reduce_items)\n",
      "    30000    0.376    0.000    0.752    0.000 _methods.py:53(_mean)\n",
      "    30000    0.888    0.000    1.434    0.000 _methods.py:86(_var)\n",
      "     4000    0.067    0.000    1.504    0.000 _ml.py:146(begin_update)\n",
      "     4000    0.013    0.000    0.255    0.000 _ml.py:178(_add_padding)\n",
      "     4000    0.021    0.000    0.071    0.000 _ml.py:546(flatten)\n",
      "     4000    0.004    0.000    0.005    0.000 _ml.py:549(<listcomp>)\n",
      "     2000    0.005    0.000    0.005    0.000 _weakrefset.py:70(__contains__)\n",
      "     2000    0.005    0.000    0.010    0.000 abc.py:180(__instancecheck__)\n",
      "18000/6000    0.071    0.000    3.000    0.001 api.py:174(begin_update)\n",
      "18000/6000    0.052    0.000    2.865    0.000 api.py:176(<listcomp>)\n",
      "    18000    0.011    0.000    0.011    0.000 api.py:179(<listcomp>)\n",
      "    18000    0.018    0.000    0.065    0.000 api.py:241(split_backward)\n",
      "    18000    0.035    0.000    0.047    0.000 api.py:248(<listcomp>)\n",
      "    36000    0.012    0.000    0.012    0.000 api.py:253(sink_return)\n",
      "36000/12000    0.089    0.000    2.847    0.000 api.py:257(wrap)\n",
      "     4000    0.037    0.000   10.944    0.003 api.py:277(begin_update)\n",
      "     4000    0.003    0.000    0.004    0.000 api.py:278(<listcomp>)\n",
      "     4000    0.034    0.000    5.660    0.001 api.py:291(predict)\n",
      "     4000    0.004    0.000    0.005    0.000 api.py:292(<listcomp>)\n",
      "     6000    0.014    0.000    0.219    0.000 api.py:330(feature_extracter_fwd)\n",
      "     6000    0.009    0.000    0.152    0.000 api.py:332(get_feats)\n",
      "     6000    0.019    0.000    0.206    0.000 api.py:337(<listcomp>)\n",
      "     6000    0.157    0.000    6.314    0.001 api.py:365(uniqued_fwd)\n",
      "12000/2000    0.058    0.000    5.377    0.003 api.py:53(predict)\n",
      "30000/6000    0.238    0.000   13.087    0.002 api.py:58(begin_update)\n",
      "     2000    0.003    0.000    0.003    0.000 arc_eager.pyx:478(finalize_doc)\n",
      "     6000    0.016    0.000    0.484    0.000 arraysetops.py:113(unique)\n",
      "     6000    0.165    0.000    0.463    0.000 arraysetops.py:256(_unique1d)\n",
      "     2432    0.016    0.000    1.835    0.001 chain.py:112(walk)\n",
      "  1209512    0.275    0.000    0.432    0.000 chain.py:15(accumulate)\n",
      "    56213    1.246    0.000    1.734    0.000 chain.py:85(move)\n",
      "    56213    0.085    0.000    1.819    0.000 chain.py:99(gen)\n",
      "     2000    0.019    0.000    0.317    0.000 check.py:132(checked_function)\n",
      "     2000    0.014    0.000    0.026    0.000 check.py:47(has_shape)\n",
      "     3687    0.002    0.000    0.002    0.000 compat.py:92(is_config)\n",
      "     8000    0.029    0.000    0.311    0.000 convolution.py:24(predict)\n",
      "    16000    0.070    0.000    0.665    0.000 convolution.py:27(begin_update)\n",
      "    16000    0.010    0.000    0.010    0.000 convolution.py:32(_get_finish_update)\n",
      "   294000    0.166    0.000    0.232    0.000 describe.py:21(__get__)\n",
      "   172000    0.389    0.000    3.269    0.000 describe.py:35(__get__)\n",
      "     2000    0.000    0.000    0.000    0.000 doc.pxd:42(__set__)\n",
      "     2000    0.026    0.000    0.042    0.000 doc.pyx:111(__init__)\n",
      "    16715    0.009    0.000    0.014    0.000 doc.pyx:189(__getitem__)\n",
      "    10458    0.002    0.000    0.002    0.000 doc.pyx:235(__len__)\n",
      "    16715    0.001    0.000    0.001    0.000 doc.pyx:41(bounds_check)\n",
      "     4000    0.011    0.000    0.018    0.000 doc.pyx:415(__get__)\n",
      "    33569    0.008    0.000    0.011    0.000 doc.pyx:542(__pyx_fuse_0push_back)\n",
      "      536    0.000    0.000    0.000    0.000 doc.pyx:542(__pyx_fuse_1push_back)\n",
      "     6000    0.005    0.000    0.133    0.000 doc.pyx:566(to_array (wrapper))\n",
      "     6000    0.082    0.000    0.128    0.000 doc.pyx:566(to_array)\n",
      "      677    0.003    0.000    0.003    0.000 doc.pyx:650(_realloc)\n",
      "     2000    0.007    0.000    0.010    0.000 doc.pyx:71(_get_chunker)\n",
      "     6000    0.030    0.000    0.119    0.000 doc.pyx:864(extend_tensor)\n",
      "     6000    0.005    0.000    0.021    0.000 fromnumeric.py:1534(nonzero)\n",
      "     2000    0.003    0.000    0.017    0.000 fromnumeric.py:1721(clip)\n",
      "     6000    0.012    0.000    0.085    0.000 fromnumeric.py:2101(cumsum)\n",
      "     2000    0.006    0.000    0.028    0.000 fromnumeric.py:2222(amax)\n",
      "   220000    0.810    0.000    2.666    0.000 fromnumeric.py:2456(prod)\n",
      "    14000    0.016    0.000    0.103    0.000 fromnumeric.py:50(_wrapfunc)\n",
      "     6000    0.061    0.000    0.071    0.000 function_base.py:1851(diff)\n",
      "    24000    0.773    0.000    2.445    0.000 hash_embed.py:48(begin_update)\n",
      "     2000    0.007    0.000    6.777    0.003 language.py:326(__call__)\n",
      "     2000    0.008    0.000    0.820    0.000 language.py:367(make_doc)\n",
      "    30000    0.462    0.000    0.462    0.000 layernorm.py:102(_forward)\n",
      "     8000    0.151    0.000    2.629    0.000 layernorm.py:43(predict)\n",
      "    22000    0.337    0.000    8.283    0.000 layernorm.py:50(begin_update)\n",
      "    22000    0.303    0.000    1.145    0.000 layernorm.py:70(_begin_update_scale_shift)\n",
      "    30000    0.236    0.000    2.697    0.000 layernorm.py:81(_get_moments)\n",
      "     4720    0.021    0.000    0.059    0.000 lemmatizer.py:19(__call__)\n",
      "     3749    0.009    0.000    0.016    0.000 lemmatizer.py:40(is_base_form)\n",
      "     3749    0.005    0.000    0.005    0.000 lemmatizer.py:46(<listcomp>)\n",
      "     1390    0.014    0.000    0.020    0.000 lemmatizer.py:90(lemmatize)\n",
      "     1140    0.008    0.000    0.012    0.000 lex_attrs.py:110(word_shape)\n",
      "     2249    0.002    0.000    0.003    0.000 lex_attrs.py:137(lower)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:138(prefix)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:139(suffix)\n",
      "     1140    0.000    0.000    0.000    0.000 lex_attrs.py:140(cluster)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:141(is_alpha)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:142(is_digit)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:143(is_lower)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:144(is_space)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:145(is_title)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:146(is_upper)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:147(is_stop)\n",
      "     1140    0.000    0.000    0.000    0.000 lex_attrs.py:148(is_oov)\n",
      "     1140    0.000    0.000    0.000    0.000 lex_attrs.py:149(get_prob)\n",
      "     1140    0.004    0.000    0.007    0.000 lex_attrs.py:15(like_num)\n",
      "     1140    0.003    0.000    0.005    0.000 lex_attrs.py:26(is_punct)\n",
      "     1140    0.003    0.000    0.004    0.000 lex_attrs.py:33(is_ascii)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:52(is_bracket)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:57(is_quote)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:62(is_left_punct)\n",
      "     1140    0.001    0.000    0.001    0.000 lex_attrs.py:67(is_right_punct)\n",
      "     1140    0.001    0.000    0.002    0.000 lex_attrs.py:72(is_currency)\n",
      "     1140    0.002    0.000    0.004    0.000 lex_attrs.py:80(like_email)\n",
      "     1140    0.004    0.000    0.006    0.000 lex_attrs.py:84(like_url)\n",
      "     8000    0.196    0.000    1.372    0.000 maxout.py:58(predict)\n",
      "    22000    0.638    0.000    4.331    0.000 maxout.py:66(begin_update)\n",
      "   172000    0.124    0.000    0.124    0.000 mem.py:25(__contains__)\n",
      "   172000    0.564    0.000    2.756    0.000 mem.py:28(__getitem__)\n",
      "     4000    0.009    0.000    2.208    0.001 model.py:124(predict)\n",
      "38000/4000    0.082    0.000    5.763    0.001 model.py:155(__call__)\n",
      "     4000    0.001    0.000    0.001    0.000 nn_parser.pyx:106(get_feat_weights)\n",
      "     4000    0.120    0.000   14.302    0.004 nn_parser.pyx:325(__call__)\n",
      "     4000    0.833    0.000    0.844    0.000 nn_parser.pyx:386(parse_batch)\n",
      "     4000    0.026    0.000   12.901    0.003 nn_parser.pyx:722(get_batch_model)\n",
      "     4000    0.038    0.000    0.169    0.000 nn_parser.pyx:774(set_annotations)\n",
      "     2000    0.000    0.000    0.000    0.000 nn_parser.pyx:802(__get__)\n",
      "     4000    0.031    0.000    1.674    0.000 nn_parser.pyx:82(__init__)\n",
      "     2000    0.001    0.000    0.031    0.000 nonproj.pyx:118(deprojectivize (wrapper))\n",
      "     2000    0.028    0.000    0.030    0.000 nonproj.pyx:118(deprojectivize)\n",
      "       32    0.002    0.000    0.002    0.000 nonproj.pyx:167(_find_new_head)\n",
      "    12000    0.007    0.000    0.072    0.000 numeric.py:424(asarray)\n",
      "   160000    0.073    0.000    0.143    0.000 numeric.py:495(asanyarray)\n",
      "    64000    0.037    0.000    0.321    0.000 numeric.py:547(ascontiguousarray)\n",
      "    44000    0.023    0.000    0.023    0.000 ops.pyx:101(dropout)\n",
      "    44000    0.006    0.000    0.006    0.000 ops.pyx:103(lambda1)\n",
      "    12000    0.110    0.000    0.114    0.000 ops.pyx:117(flatten)\n",
      "    12000    0.069    0.000    0.069    0.000 ops.pyx:135(unflatten)\n",
      "    24000    0.011    0.000    0.011    0.000 ops.pyx:150(get_dropout_mask)\n",
      "    48000    0.280    0.000    0.923    0.000 ops.pyx:158(allocate)\n",
      "    70000    0.331    0.000    0.357    0.000 ops.pyx:168(asarray)\n",
      "     2000    0.051    0.000    0.108    0.000 ops.pyx:203(softmax)\n",
      "    32000    2.937    0.000    2.937    0.000 ops.pyx:333(batch_dot)\n",
      "     2000    0.017    0.000    0.059    0.000 ops.pyx:352(affine)\n",
      "    30000    0.491    0.000    0.497    0.000 ops.pyx:419(maxout)\n",
      "    24000    0.265    0.000    0.777    0.000 ops.pyx:452(seq2col)\n",
      "    24000    0.092    0.000    0.540    0.000 ops.pyx:518(hash)\n",
      "     2000    0.015    0.000    5.972    0.003 pipeline.pyx:420(__call__)\n",
      "     2000    0.025    0.000    5.788    0.003 pipeline.pyx:432(predict)\n",
      "     2000    0.086    0.000    0.170    0.000 pipeline.pyx:443(set_annotations)\n",
      "     2000    0.002    0.000    0.002    0.000 pipeline.pyx:921(__get__)\n",
      "     8000    0.037    0.000    3.033    0.000 resnet.py:14(__call__)\n",
      "    16000    0.081    0.000    6.541    0.000 resnet.py:17(begin_update)\n",
      "    44000    0.078    0.000    0.128    0.000 shape_base.py:11(atleast_1d)\n",
      "     4000    0.011    0.000    0.151    0.000 shape_base.py:182(vstack)\n",
      "     4000    0.008    0.000    0.036    0.000 shape_base.py:234(<listcomp>)\n",
      "    22000    0.040    0.000    0.335    0.000 shape_base.py:236(hstack)\n",
      "    22000    0.033    0.000    0.160    0.000 shape_base.py:283(<listcomp>)\n",
      "     8000    0.018    0.000    0.029    0.000 shape_base.py:63(atleast_2d)\n",
      "     2000    0.015    0.000    0.253    0.000 softmax.py:15(predict)\n",
      "    78000    0.122    0.000    0.122    0.000 stringsource:343(__cinit__)\n",
      "    78000    0.015    0.000    0.015    0.000 stringsource:370(__dealloc__)\n",
      "    78000    0.074    0.000    0.196    0.000 stringsource:645(memoryview_cwrapper)\n",
      "    78000    0.015    0.000    0.015    0.000 stringsource:651(memoryview_check)\n",
      "     2432    0.016    0.000    1.863    0.001 text.py:150(make_sentence)\n",
      "     2000    0.011    0.000    1.876    0.001 text.py:195(make_short_sentence)\n",
      "     2432    0.003    0.000    0.006    0.000 text.py:91(word_join)\n",
      "    16715    0.004    0.000    0.004    0.000 token.pxd:18(cinit)\n",
      "    40627    0.017    0.000    0.024    0.000 tokenizer.pyx:140(_try_cache)\n",
      "    11303    0.014    0.000    0.715    0.000 tokenizer.pyx:153(_tokenize)\n",
      "    11303    0.180    0.000    0.395    0.000 tokenizer.pyx:165(_split_affixes)\n",
      "    11303    0.098    0.000    0.304    0.000 tokenizer.pyx:216(_attach_tokens)\n",
      "    11303    0.002    0.000    0.002    0.000 tokenizer.pyx:275(_save_cached)\n",
      "    10045    0.083    0.000    0.083    0.000 tokenizer.pyx:293(find_infix)\n",
      "    15105    0.016    0.000    0.016    0.000 tokenizer.pyx:305(find_prefix)\n",
      "    15105    0.193    0.000    0.193    0.000 tokenizer.pyx:317(find_suffix)\n",
      "     2000    0.033    0.000    0.812    0.000 tokenizer.pyx:74(__call__)\n",
      "    18000    0.006    0.000    0.006    0.000 util.py:11(<lambda>)\n",
      "     4000    0.002    0.000    0.002    0.000 util.py:243(get_cuda_stream)\n",
      "     1140    0.002    0.000    0.003    0.000 util.py:322(_get_attr_unless_lookup)\n",
      "     2000    0.003    0.000    0.003    0.000 util.py:41(get_lang_class)\n",
      "     2000    0.009    0.000    0.010    0.000 util.py:46(copy_array)\n",
      "    14380    0.025    0.000    0.122    0.000 vocab.pyx:117(get)\n",
      "     1140    0.039    0.000    0.097    0.000 vocab.pyx:150(_new_lexeme)\n",
      "       12    0.000    0.000    0.000    0.000 vocab.pyx:177(_add_lex_to_vocab)\n",
      "     2000    0.005    0.000    0.006    0.000 vocab.pyx:62(__get__)\n",
      "    56213    0.037    0.000    0.037    0.000 {built-in method _bisect.bisect}\n",
      "  1101950    0.138    0.000    0.138    0.000 {built-in method _operator.add}\n",
      "      2/1    0.000    0.000    0.021    0.021 {built-in method builtins.exec}\n",
      "    40000    0.024    0.000    0.028    0.000 {built-in method builtins.getattr}\n",
      "    12432    0.011    0.000    0.011    0.000 {built-in method builtins.hasattr}\n",
      "   166000    0.057    0.000    0.067    0.000 {built-in method builtins.isinstance}\n",
      "   120000    0.032    0.000    0.032    0.000 {built-in method builtins.issubclass}\n",
      "    53781    0.011    0.000    0.011    0.000 {built-in method builtins.iter}\n",
      "    97575    0.018    0.000    0.018    0.000 {built-in method builtins.len}\n",
      "    30000    0.029    0.000    0.029    0.000 {built-in method builtins.max}\n",
      "     2000    0.002    0.000    0.002    0.000 {built-in method builtins.min}\n",
      "57780/2000    0.014    0.000   21.351    0.011 {built-in method builtins.next}\n",
      "     8405    0.001    0.000    0.001    0.000 {built-in method builtins.ord}\n",
      "     7939    0.006    0.000    0.006    0.000 {built-in method math.floor}\n",
      "   236000    0.418    0.000    0.418    0.000 {built-in method numpy.core.multiarray.array}\n",
      "    38000    0.302    0.000    0.302    0.000 {built-in method numpy.core.multiarray.concatenate}\n",
      "     4000    1.066    0.000    1.066    0.000 {built-in method numpy.core.multiarray.dot}\n",
      "     6000    0.013    0.000    0.013    0.000 {built-in method numpy.core.multiarray.empty}\n",
      "     6000    0.005    0.000    0.005    0.000 {built-in method numpy.core.multiarray.normalize_axis_index}\n",
      "     2340    0.002    0.000    0.002    0.000 {built-in method unicodedata.category}\n",
      "     6496    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}\n",
      "     1071    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "   175035    0.031    0.000    0.031    0.000 {method 'append' of 'list' objects}\n",
      "     6000    0.033    0.000    0.033    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "     2000    0.009    0.000    0.009    0.000 {method 'clip' of 'numpy.ndarray' objects}\n",
      "     1139    0.001    0.000    0.001    0.000 {method 'count' of 'str' objects}\n",
      "     6000    0.063    0.000    0.063    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "     9860    0.003    0.000    0.003    0.000 {method 'endswith' of 'str' objects}\n",
      "     1490    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "     6000    0.008    0.000    0.008    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "   319026    0.073    0.000    0.073    0.000 {method 'get' of 'dict' objects}\n",
      "    10340    0.002    0.000    0.002    0.000 {method 'isalpha' of 'str' objects}\n",
      "     2478    0.000    0.000    0.000    0.000 {method 'isdigit' of 'str' objects}\n",
      "     1140    0.000    0.000    0.000    0.000 {method 'islower' of 'str' objects}\n",
      "     1140    0.000    0.000    0.000    0.000 {method 'isspace' of 'str' objects}\n",
      "     1140    0.000    0.000    0.000    0.000 {method 'istitle' of 'str' objects}\n",
      "     9347    0.001    0.000    0.001    0.000 {method 'isupper' of 'str' objects}\n",
      "    55781    0.010    0.000    0.010    0.000 {method 'items' of 'dict' objects}\n",
      "     3572    0.004    0.000    0.004    0.000 {method 'join' of 'str' objects}\n",
      "     8108    0.003    0.000    0.003    0.000 {method 'lower' of 'str' objects}\n",
      "     1140    0.002    0.000    0.002    0.000 {method 'match' of '_regex.Pattern' objects}\n",
      "    30000    0.045    0.000    0.797    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
      "     6000    0.009    0.000    0.009    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
      "     2000    0.018    0.000    0.018    0.000 {method 'poisson' of 'mtrand.RandomState' objects}\n",
      "    56213    0.009    0.000    0.009    0.000 {method 'random' of '_random.Random' objects}\n",
      "   338000    2.820    0.000    2.820    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "     5967    0.004    0.000    0.004    0.000 {method 'replace' of 'str' objects}\n",
      "   276000    0.257    0.000    0.257    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'rsplit' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "     4618    0.002    0.000    0.002    0.000 {method 'startswith' of 'str' objects}\n",
      "    24000    0.031    0.000    0.450    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "     6000    0.006    0.000    0.139    0.000 {method 'to_array' of 'spacy.tokens.doc.Doc' objects}\n",
      "    18475    0.031    0.000    0.031    0.000 {method 'uniform' of 'mtrand.RandomState' objects}\n",
      "    30000    0.055    0.000    1.489    0.000 {method 'var' of 'numpy.ndarray' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x15a800518>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "p = pstats.Stats('generatestats')\n",
    "p.strip_dirs().sort_stats(-1).print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
