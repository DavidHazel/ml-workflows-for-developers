{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating machine learning models\n",
    "\n",
    "Machine learning involves automatically learning how to compute functions from examples.  There are several ways that this process can go wrong, including:\n",
    "\n",
    "0. Overfitting the training examples,\n",
    "1. Optimizing for the wrong objective,\n",
    "2. Starting with the wrong features,\n",
    "3. Data drift (treated in another notebook),\n",
    "4. ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet(\"data/training.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "The first concern we'd like to address is _overfitting_, in which we have a model whose performance on training examples is materially different from its performance in production.  We'll see that in action with a simple example.  First, let's choose some of our data as training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7149</th>\n",
       "      <td>7149</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>Never had any day passed so quickly! Smells ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14554</th>\n",
       "      <td>14554</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>Their respectability was as dear to her of all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>10999</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>Lady Bertram, who assured him, as soon as he k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>5891</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>It doesn't deserve one or any part of what had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27436</th>\n",
       "      <td>7436</td>\n",
       "      <td>spam</td>\n",
       "      <td>Try it for yourself in this case. I can't reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25857</th>\n",
       "      <td>5857</td>\n",
       "      <td>spam</td>\n",
       "      <td>Not to mention a lot healthier all the way for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17794</th>\n",
       "      <td>17794</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>It will last you for yeeeaars.With a good popc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14613</th>\n",
       "      <td>14613</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>She wondered, and questioned him eagerly; but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11113</th>\n",
       "      <td>11113</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>Her report was highly favourable. Crawford's i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16898</th>\n",
       "      <td>16898</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>Mrs. Norris could tolerate its being for you h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index       label                                               text\n",
       "7149    7149  legitimate  Never had any day passed so quickly! Smells ba...\n",
       "14554  14554  legitimate  Their respectability was as dear to her of all...\n",
       "10999  10999  legitimate  Lady Bertram, who assured him, as soon as he k...\n",
       "5891    5891  legitimate  It doesn't deserve one or any part of what had...\n",
       "27436   7436        spam  Try it for yourself in this case. I can't reco...\n",
       "25857   5857        spam  Not to mention a lot healthier all the way for...\n",
       "17794  17794  legitimate  It will last you for yeeeaars.With a good popc...\n",
       "14613  14613  legitimate  She wondered, and questioned him eagerly; but ...\n",
       "11113  11113  legitimate  Her report was highly favourable. Crawford's i...\n",
       "16898  16898  legitimate  Mrs. Norris could tolerate its being for you h..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overfit_training = data.sample(1000)\n",
    "overfit_training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll \"train\" a very simple \"model\" from these examples:  we'll just memorize hashes of every example so we can look up whether a given example is legitimate or not.  We'll program defensively, too:  if we don't find an example in either set, we'll call it legitimate if it has an even number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OverfittingSpamModel(object):\n",
    "    def __init__(self):\n",
    "        self.legit = set()\n",
    "        self.spam = set()\n",
    "    \n",
    "    def fit(self, df):\n",
    "        for tup in df.itertuples():\n",
    "            if tup.label == \"legitimate\":\n",
    "                self.legit.add(hash(tup.text))\n",
    "            else:\n",
    "                self.spam.add(hash(tup.text))\n",
    "    \n",
    "    def predict(self, text):\n",
    "        h = hash(text)\n",
    "        if h in self.legit:\n",
    "            return \"legitimate\"\n",
    "        elif h in self.spam:\n",
    "            return \"spam\"\n",
    "        else:\n",
    "            return (len(text) % 2 == 0) and \"legitimate\" or \"spam\"\n",
    "\n",
    "osm = OverfittingSpamModel()\n",
    "osm.fit(overfit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this out with some of our training examples to see how well it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text is 'I've given him a mix...': actual label is spam; predicted label is spam\n",
      "text is 'Much too little caff...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'Even if they are Pro...': actual label is spam; predicted label is spam\n",
      "text is 'I wish that big jug ...': actual label is spam; predicted label is spam\n",
      "text is 'He dined with us the...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'I promised them so f...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'This product provide...': actual label is spam; predicted label is spam\n",
      "text is 'As soon as we came t...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'How they could get t...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'Friday Evening Lady ...': actual label is legitimate; predicted label is legitimate\n"
     ]
    }
   ],
   "source": [
    "for row in overfit_training.sample(10).itertuples():\n",
    "    print(\"text is '%s...': actual label is %s; predicted label is %s\" % (row.text[0:20], row.label, osm.predict(row.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model appears to work really well!  We can test it on the whole set of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(osm, df):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for row in df.itertuples():\n",
    "        if row.label == osm.predict(row.text):\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    \n",
    "    if correct + incorrect == 0:\n",
    "        return 100\n",
    "    \n",
    "    return (float(correct) / float(correct + incorrect) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(osm, overfit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is enormously successful!  It has one hundred percent accuracy.  We probably expected this result, but it's always nice when things work out as you expected they would.  Let's see how well our model has generalized to data it _hasn't_ seen by testing it on the rest of our dataset (39,000 more examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.6725"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(osm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh!  It appears that our model is not much better than a coin toss once it's running on data it hasn't already seen.  If we had put this model in production, our application surely wouldn't have performed well.\n",
    "\n",
    "We want a way to identify this problem before we put a model into production, and you've essentially seen it already:  when we train a model, we don't use all of the data we have available to us.  Instead, we divide our examples into distinct _training_ and _test_ sets, usually with about 70% of the examples in the former and 30% in the latter.  \n",
    "The training algorithm only considers the examples in the training set.  After training our model, we can evaluate its performance on both the training set (which it saw during training) and the test set (which it didn't).  If the performance is materially different on the different sets, we know that we've overfit the data when training our model.\n",
    "\n",
    "Next up, we'll deal with the question of what metrics we should use to evaluate our performance.  We used accuracy above, but is it always the best option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics and types of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
