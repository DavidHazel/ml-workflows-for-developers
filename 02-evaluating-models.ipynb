{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating machine learning models\n",
    "\n",
    "Machine learning involves automatically learning how to compute functions from examples.  There are several ways that this process can go wrong, including:\n",
    "\n",
    "0. Overfitting to the training examples,\n",
    "1. Optimizing for the wrong objective,\n",
    "2. Starting with the wrong features,\n",
    "3. Data drift (treated in another notebook),\n",
    "4. ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet(\"data/training.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "The first concern we'd like to address is _overfitting_, in which we have a model whose performance on training examples is materially different from its performance in production.  We'll see that in action with a simple example.  First, let's choose some of our data as training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31728</th>\n",
       "      <td>11728</td>\n",
       "      <td>spam</td>\n",
       "      <td>All I found was 1/10 the size and convenience....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28660</th>\n",
       "      <td>8660</td>\n",
       "      <td>spam</td>\n",
       "      <td>I'm not really *that* picky.No texture, or mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580</th>\n",
       "      <td>3580</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>With their wealth, their views increased; thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21472</th>\n",
       "      <td>1472</td>\n",
       "      <td>spam</td>\n",
       "      <td>It reminded me of kombucha, in a bad way. This...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12507</th>\n",
       "      <td>12507</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>I was hoping that the new recipe is terrible! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12791</th>\n",
       "      <td>12791</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>She had thought her wretchedly altered, and in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16693</th>\n",
       "      <td>16693</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>I wanted to throw up pieces of paper. Harriet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25684</th>\n",
       "      <td>5684</td>\n",
       "      <td>spam</td>\n",
       "      <td>As soon as you receive your merchandise. They ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>741</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>You will not be sorry when it was no worse, sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27499</th>\n",
       "      <td>7499</td>\n",
       "      <td>spam</td>\n",
       "      <td>I definitely feel more limber and alert when I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index       label                                               text\n",
       "31728  11728        spam  All I found was 1/10 the size and convenience....\n",
       "28660   8660        spam  I'm not really *that* picky.No texture, or mor...\n",
       "3580    3580  legitimate  With their wealth, their views increased; thei...\n",
       "21472   1472        spam  It reminded me of kombucha, in a bad way. This...\n",
       "12507  12507  legitimate  I was hoping that the new recipe is terrible! ...\n",
       "12791  12791  legitimate  She had thought her wretchedly altered, and in...\n",
       "16693  16693  legitimate  I wanted to throw up pieces of paper. Harriet ...\n",
       "25684   5684        spam  As soon as you receive your merchandise. They ...\n",
       "741      741  legitimate  You will not be sorry when it was no worse, sh...\n",
       "27499   7499        spam  I definitely feel more limber and alert when I..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overfit_training = data.sample(1000)\n",
    "overfit_training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll \"train\" a very simple \"model\" from these examples:  we'll just memorize hashes of every example so we can look up whether a given example is legitimate or not.  We'll program defensively, too:  if we don't find an example in either set, we'll call it legitimate if it has an even number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OverfittingSpamModel(object):\n",
    "    def __init__(self):\n",
    "        self.legit = set()\n",
    "        self.spam = set()\n",
    "    \n",
    "    def fit(self, df):\n",
    "        for tup in df.itertuples():\n",
    "            if tup.label == \"legitimate\":\n",
    "                self.legit.add(hash(tup.text))\n",
    "            else:\n",
    "                self.spam.add(hash(tup.text))\n",
    "    \n",
    "    def predict(self, text):\n",
    "        h = hash(text)\n",
    "        if h in self.legit:\n",
    "            return \"legitimate\"\n",
    "        elif h in self.spam:\n",
    "            return \"spam\"\n",
    "        else:\n",
    "            return (len(text) % 2 == 0) and \"legitimate\" or \"spam\"\n",
    "\n",
    "osm = OverfittingSpamModel()\n",
    "osm.fit(overfit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this out with some of our training examples to see how well it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text is 'True, upon my honour...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'When I first heard a...': actual label is spam; predicted label is spam\n",
      "text is 'Tinyada doesn't get ...': actual label is spam; predicted label is spam\n",
      "text is 'How have I been to y...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'I use it daily on al...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'I love the convenien...': actual label is spam; predicted label is spam\n",
      "text is 'He begged to know fu...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'Hardly any taste. Af...': actual label is spam; predicted label is spam\n",
      "text is 'He could tell her no...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'But if she _did_, th...': actual label is legitimate; predicted label is legitimate\n"
     ]
    }
   ],
   "source": [
    "for row in overfit_training.sample(10).itertuples():\n",
    "    print(\"text is '%s...': actual label is %s; predicted label is %s\" % (row.text[0:20], row.label, osm.predict(row.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model appears to work really well!  We can test it on the whole set of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(osm, df):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for row in df.itertuples():\n",
    "        if row.label == osm.predict(row.text):\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    \n",
    "    if correct + incorrect == 0:\n",
    "        return 100\n",
    "    \n",
    "    return (float(correct) / float(correct + incorrect) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(osm, overfit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is enormously successful!  It has one hundred percent accuracy.  We probably expected this result, but it's always nice when things work out as you expected they would.  Let's see how well our model has generalized to data it _hasn't_ seen by testing it on the rest of our dataset (39,000 more examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.67"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(osm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh!  It appears that our model is not much better than a coin toss once it's running on data it hasn't already seen.  If we had put this model in production, our application surely wouldn't have performed well.\n",
    "\n",
    "We want a way to identify this problem before we put a model into production, and you've essentially seen it already:  when we train a model, we don't use all of the data we have available to us.  Instead, we divide our examples into distinct _training_ and _test_ sets, usually with about 70% of the examples in the former and 30% in the latter.  \n",
    "The training algorithm only considers the examples in the training set.  After training our model, we can evaluate its performance on both the training set (which it saw during training) and the test set (which it didn't).  If the performance is materially different on the different sets, we know that we've overfit the data when training our model.\n",
    "\n",
    "Next up, we'll deal with the question of what metrics we should use to evaluate our performance.  We used accuracy above, but is it always the best option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics and types of error\n",
    "\n",
    "Our training data set is _balanced_ between classes -- there are equal numbers of legitimate and spam documents.  But data in the real world are typically not balanced, and are often wildly unbalanced.  For example:\n",
    "\n",
    "- The worldwide incidence of Rh-negative blood types is approximiately 6 percent;\n",
    "- Between one and three percent of actual consumer payments transactions are fraudulent; and\n",
    "- A rare disease may have an incidence rate on the order of one in ten thousand per year.\n",
    "\n",
    "In cases like these, it would be possible to develop an accurate model that wouldn't produce meaningful results; for example:\n",
    "\n",
    "- A blood type tester that always returned \"Rh-positive\" would be accurate roughly 94% of the time on a sufficiently diverse population;\n",
    "- A fraud detector that always returned \"not fradulent\" would be accurate between 97-99% of the time -- until, that is, fraudsters determined that their charges would likely go through, increasing the rate of fraudulent charges; and\n",
    "- A technique to screen for a very rare disease could be quite accurate by simply never identifying disease.\n",
    "\n",
    "In many applications, we're not only interested in correctly identifying members of one class, we're interested in correctly identifying members of both classes.  We can capture this behavior by using better metrics than accuracy.\n",
    "\n",
    "To learn about these metrics, let's start with an unbalanced data set, in which 90% of the messages are spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_sample = data[data.label == 'legitimate'].sample(2000)\n",
    "spam_sample = data[data.label == 'spam'].sample(18000)\n",
    "unbalanced = pd.DataFrame.append(legit_sample, spam_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting, we'll split the unbalanced data set into training and test sets, using functionality from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "unbalanced_train, unbalanced_test = train_test_split(unbalanced, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create a simple model that should work pretty well for spam messages but not necessarily as well for legitimate ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "    \n",
    "class SensitiveSpamModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.legit = set()\n",
    "        self.spam = set()\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\" Train a model based on the most frequent unique \n",
    "            words in each class of documents \"\"\"\n",
    "        legit_words = defaultdict(lambda: 0)\n",
    "        spam_words = defaultdict(lambda: 0)\n",
    "        \n",
    "        for tup in df.itertuples():\n",
    "            target = spam_words\n",
    "            if tup.label == \"legitimate\":\n",
    "                target = legit_words\n",
    "            for word in re.split(r\"\\W+\", tup.text):\n",
    "                if len(word) > 0:\n",
    "                    target[word.lower()] += 1\n",
    "        \n",
    "        # remove words common to both classes\n",
    "        for word in set(legit_words.keys()).intersection(set(spam_words.keys())):\n",
    "            del legit_words[word]\n",
    "            del spam_words[word]\n",
    "        \n",
    "        top_legit_words = sorted(legit_words.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        top_spam_words = sorted(spam_words.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        \n",
    "        # store ten times as many words from the spam set\n",
    "        self.legit = set([t[0] for t in top_legit_words[:100]])\n",
    "        self.spam = set([t[0] for t in top_spam_words[:1000]])\n",
    "            \n",
    "    def predict(self, text):\n",
    "        legit_score = 0\n",
    "        spam_score = 0\n",
    "        \n",
    "        for word in re.split(r\"\\W+\", text):\n",
    "            w = word.lower()\n",
    "            if word in self.legit:\n",
    "                legit_score = legit_score + 1\n",
    "            elif word in self.spam:\n",
    "                spam_score = spam_score + 1\n",
    "        \n",
    "        # bias results towards spam in the event of ties\n",
    "        return (legit_score > spam_score) and \"legitimate\" or \"spam\"\n",
    "\n",
    "ssm = SensitiveSpamModel()\n",
    "ssm.fit(unbalanced_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy on our training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.30714285714285"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(ssm, unbalanced_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we've not overfit our training sample, let's check the accuracy on the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(ssm, unbalanced_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not quite as good as the results on our training sample, but it's still pretty decent (that is, it's better than just always returning \"spam\" would be given the balance of the classes).  \n",
    "\n",
    "However, we get a different picture if we look at our model's performance on the balanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.4725"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(ssm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the accuracy is even worse if we look at a sample where the label balance is reversed (i.e., only 10% of documents are spam):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit_sample = data[data.label == 'legitimate'].sample(900)\n",
    "spam_sample = data[data.label == 'spam'].sample(100)\n",
    "legit_biased = pd.DataFrame.append(legit_sample, spam_sample)\n",
    "model_accuracy(ssm, legit_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to understand the performance of our model with some metric that captures not only the overall accuracy but the accuracy for positive cases and the accuracy for negative cases.  That is, if we assume that our goal is to identify spam documents, we care about:\n",
    "\n",
    "- _true positives_, which are spam documents that our model predicts as spam;\n",
    "- _true negatives_, which are legitimate documents that our model predicts as legitimate;\n",
    "- _false positives_, which are legitimate documents that our model predicts as spam; and\n",
    "- _false negatives_, which are spam documents that our model predicts as legitimate\n",
    "\n",
    "The proportions between these quantities can provide interesting metrics.  For example, the ratio of true positives to actual positives (that is, true positives + false negatives) is called _recall_, which indicates the percentage of spam documents we've selected.  The ratio of true positives to all predicted positives (that is, true positives + false positives) is called _precision_, which indicates the percentage of predicted spam documents that are actually spam.  Ideally, a good classifier would have both high precision and high recall, but in some applications either precision or recall is more important.\n",
    "\n",
    "We can visualize the overall performance of a classifier with a _confusion matrix_, which plots actual labels in rows and predicted labels in columns.  The confusion matrix thus puts correct predictions along one diagonal and various kinds of incorrect predictions elsewhere.  Let's see one in action for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import plot\n",
    "\n",
    "def true_and_predicted(df, model):\n",
    "    return (df.label.values, [model.predict(txt) for txt in df.text.values])\n",
    "\n",
    "df, chart = plot.binary_confusion_matrix(*true_and_predicted(unbalanced, ssm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}}, \"data\": {\"name\": \"data-053978ecae3201d3b405827a497b9939\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"raw_count\"}], \"x\": {\"type\": \"ordinal\", \"field\": \"predicted\"}, \"y\": {\"type\": \"ordinal\", \"field\": \"actual\"}}, \"height\": 215, \"width\": 215, \"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\", \"datasets\": {\"data-053978ecae3201d3b405827a497b9939\": [{\"actual\": \"legitimate\", \"predicted\": \"legitimate\", \"raw_count\": 783, \"value\": 0.3915}, {\"actual\": \"spam\", \"predicted\": \"legitimate\", \"raw_count\": 1217, \"value\": 0.6085}, {\"actual\": \"legitimate\", \"predicted\": \"spam\", \"raw_count\": 6, \"value\": 0.0003333333333333333}, {\"actual\": \"spam\", \"predicted\": \"spam\", \"raw_count\": 17994, \"value\": 0.9996666666666667}]}};\n",
       "var opt = {};\n",
       "var type = \"vega-lite\";\n",
       "var id = \"59edab5f-053e-4798-a00c-2ec22fab1ad4\";\n",
       "\n",
       "var output_area = this;\n",
       "\n",
       "require([\"nbextensions/jupyter-vega/index\"], function(vega) {\n",
       "  var target = document.createElement(\"div\");\n",
       "  target.id = id;\n",
       "  target.className = \"vega-embed\";\n",
       "\n",
       "  var style = document.createElement(\"style\");\n",
       "  style.textContent = [\n",
       "    \".vega-embed .error p {\",\n",
       "    \"  color: firebrick;\",\n",
       "    \"  font-size: 14px;\",\n",
       "    \"}\",\n",
       "  ].join(\"\\\\n\");\n",
       "\n",
       "  // element is a jQuery wrapped DOM element inside the output area\n",
       "  // see http://ipython.readthedocs.io/en/stable/api/generated/\\\n",
       "  // IPython.display.html#IPython.display.Javascript.__init__\n",
       "  element[0].appendChild(target);\n",
       "  element[0].appendChild(style);\n",
       "\n",
       "  vega.render(\"#\" + id, spec, type, opt, output_area);\n",
       "}, function (err) {\n",
       "  if (err.requireType !== \"scripterror\") {\n",
       "    throw(err);\n",
       "  }\n",
       "});\n"
      ],
      "text/plain": [
       "<vega.vegalite.VegaLite at 0x1a10ede668>"
      ]
     },
     "metadata": {
      "jupyter-vega": "#59edab5f-053e-4798-a00c-2ec22fab1ad4"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {
      "jupyter-vega": "#59edab5f-053e-4798-a00c-2ec22fab1ad4"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>raw_count</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>783</td>\n",
       "      <td>0.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>1217</td>\n",
       "      <td>0.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>spam</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>17994</td>\n",
       "      <td>0.999667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual   predicted  raw_count     value\n",
       "0  legitimate  legitimate        783  0.391500\n",
       "1        spam  legitimate       1217  0.608500\n",
       "2  legitimate        spam          6  0.000333\n",
       "3        spam        spam      17994  0.999667"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
