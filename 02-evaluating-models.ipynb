{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating machine learning models\n",
    "\n",
    "Machine learning involves automatically learning how to compute functions from examples.  There are several ways that this process can go wrong, including:\n",
    "\n",
    "0. Overfitting the training examples,\n",
    "1. Optimizing for the wrong objective,\n",
    "2. Starting with the wrong features,\n",
    "3. Data drift (treated in another notebook),\n",
    "4. ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet(\"data/training.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "The first concern we'd like to address is _overfitting_, in which we have a model whose performance on training examples is materially different from its performance in production.  We'll see that in action with a simple example.  First, let's choose some of our data as training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33630</th>\n",
       "      <td>13630</td>\n",
       "      <td>spam</td>\n",
       "      <td>It ain't healthy but darn is it good. The soup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14768</th>\n",
       "      <td>14768</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>No second attachment, the only thoroughly natu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34240</th>\n",
       "      <td>14240</td>\n",
       "      <td>spam</td>\n",
       "      <td>It's possible that I'll eventually figure out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30075</th>\n",
       "      <td>10075</td>\n",
       "      <td>spam</td>\n",
       "      <td>IT IS THE SKELETON PAPER CUPS! I absolutely sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24878</th>\n",
       "      <td>4878</td>\n",
       "      <td>spam</td>\n",
       "      <td>CANNOT BEAT IT! Some years ago I switched to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27309</th>\n",
       "      <td>7309</td>\n",
       "      <td>spam</td>\n",
       "      <td>When I got this item today and I received it o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33151</th>\n",
       "      <td>13151</td>\n",
       "      <td>spam</td>\n",
       "      <td>And they don't leave stains and three the pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>1232</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>I can not express how much I felt. She could s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16593</th>\n",
       "      <td>16593</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>They remained together at the Park soon after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38255</th>\n",
       "      <td>18255</td>\n",
       "      <td>spam</td>\n",
       "      <td>The vanilla cookie/cracker with creme de cocoa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index       label                                               text\n",
       "33630  13630        spam  It ain't healthy but darn is it good. The soup...\n",
       "14768  14768  legitimate  No second attachment, the only thoroughly natu...\n",
       "34240  14240        spam  It's possible that I'll eventually figure out ...\n",
       "30075  10075        spam  IT IS THE SKELETON PAPER CUPS! I absolutely sw...\n",
       "24878   4878        spam  CANNOT BEAT IT! Some years ago I switched to t...\n",
       "27309   7309        spam  When I got this item today and I received it o...\n",
       "33151  13151        spam  And they don't leave stains and three the pric...\n",
       "1232    1232  legitimate  I can not express how much I felt. She could s...\n",
       "16593  16593  legitimate  They remained together at the Park soon after ...\n",
       "38255  18255        spam  The vanilla cookie/cracker with creme de cocoa..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overfit_training = data.sample(1000)\n",
    "overfit_training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll \"train\" a very simple \"model\" from these examples:  we'll just memorize hashes of every example so we can look up whether a given example is legitimate or not.  We'll program defensively, too:  if we don't find an example in either set, we'll call it legitimate if it has an even number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OverfittingSpamModel(object):\n",
    "    def __init__(self):\n",
    "        self.legit = set()\n",
    "        self.spam = set()\n",
    "    \n",
    "    def fit(self, df):\n",
    "        for tup in df.itertuples():\n",
    "            if tup.label == \"legitimate\":\n",
    "                self.legit.add(hash(tup.text))\n",
    "            else:\n",
    "                self.spam.add(hash(tup.text))\n",
    "    \n",
    "    def predict(self, text):\n",
    "        h = hash(text)\n",
    "        if h in self.legit:\n",
    "            return \"legitimate\"\n",
    "        elif h in self.spam:\n",
    "            return \"spam\"\n",
    "        else:\n",
    "            return (len(text) % 2 == 0) and \"legitimate\" or \"spam\"\n",
    "\n",
    "osm = OverfittingSpamModel()\n",
    "osm.fit(overfit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this out with some of our training examples to see how well it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text is 'I have two dogs and ...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'This is my new hambu...': actual label is spam; predicted label is spam\n",
      "text is 'Well, I was so aston...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'You were disgusted w...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'On this product they...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'The two friends who ...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'And keeps coffee tab...': actual label is spam; predicted label is spam\n",
      "text is 'They were too much i...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'But they have a rich...': actual label is legitimate; predicted label is legitimate\n",
      "text is 'I used these k-cups....': actual label is spam; predicted label is spam\n"
     ]
    }
   ],
   "source": [
    "for row in overfit_training.sample(10).itertuples():\n",
    "    print(\"text is '%s...': actual label is %s; predicted label is %s\" % (row.text[0:20], row.label, osm.predict(row.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model appears to work really well!  We can test it on the whole set of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(osm, df):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for row in df.itertuples():\n",
    "        if row.label == osm.predict(row.text):\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    \n",
    "    if correct + incorrect == 0:\n",
    "        return 100\n",
    "    \n",
    "    return (float(correct) / float(correct + incorrect) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(osm, overfit_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is enormously successful!  It has one hundred percent accuracy.  We probably expected this result, but it's always nice when things work out as you expected they would.  Let's see how well our model has generalized to data it _hasn't_ seen by testing it on the rest of our dataset (39,000 more examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.60249999999999"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(osm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh!  It appears that our model is not much better than a coin toss once it's running on data it hasn't already seen.  If we had put this model in production, our application surely wouldn't have performed well.\n",
    "\n",
    "We want a way to identify this problem before we put a model into production, and you've essentially seen it already:  when we train a model, we don't use all of the data we have available to us.  Instead, we divide our examples into distinct _training_ and _test_ sets, usually with about 70% of the examples in the former and 30% in the latter.  \n",
    "The training algorithm only considers the examples in the training set.  After training our model, we can evaluate its performance on both the training set (which it saw during training) and the test set (which it didn't).  If the performance is materially different on the different sets, we know that we've overfit the data when training our model.\n",
    "\n",
    "Next up, we'll deal with the question of what metrics we should use to evaluate our performance.  We used accuracy above, but is it always the best option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics and types of error\n",
    "\n",
    "Our training data set is _balanced_ between classes -- there are equal numbers of legitimate and spam documents.  But data in the real world are typically not balanced, and are often wildly unbalanced.  For example:\n",
    "\n",
    "- The worldwide incidence of Rh-negative blood types is approximiately 6 percent;\n",
    "- Between one and three percent of actual consumer payments transactions are fraudulent; and\n",
    "- A rare disease may have an incidence rate on the order of one in ten thousand per year.\n",
    "\n",
    "In cases like these, it would be possible to develop an accurate model that wouldn't produce meaningful results; for example:\n",
    "\n",
    "- A blood type tester that always returned \"Rh-positive\" would be accurate roughly 94% of the time on a sufficiently diverse population;\n",
    "- A fraud detector that always returned \"not fradulent\" would be accurate between 97-99% of the time -- until, that is, fraudsters determined that their charges would likely go through, increasing the rate of fraudulent charges; and\n",
    "- A technique to screen for a very rare disease could be quite accurate by simply never identifying disease.\n",
    "\n",
    "In many applications, we're not only interested in correctly identifying members of one class, we're interested in correctly identifying members of both classes.  We can capture this behavior by using better metrics than accuracy.\n",
    "\n",
    "To learn about these metrics, let's start with an unbalanced data set, in which 90% of the messages are spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "legit_sample = data[data.label == 'legitimate'].sample(2000)\n",
    "spam_sample = data[data.label == 'spam'].sample(18000)\n",
    "unbalanced = pd.DataFrame.append(legit_sample, spam_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting, we'll split the unbalanced data set into training and test sets, using functionality from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "unbalanced_train, unbalanced_test = train_test_split(unbalanced, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create a simple model that should work pretty well for spam messages but not necessarily as well for legitimate ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "    \n",
    "class SensitiveSpamModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.legit = set()\n",
    "        self.spam = set()\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\" Train a model based on the most frequent unique \n",
    "            words in each class of documents \"\"\"\n",
    "        legit_words = defaultdict(lambda: 0)\n",
    "        spam_words = defaultdict(lambda: 0)\n",
    "        \n",
    "        for tup in df.itertuples():\n",
    "            target = spam_words\n",
    "            if tup.label == \"legitimate\":\n",
    "                target = legit_words\n",
    "            for word in re.split(r\"\\W+\", tup.text):\n",
    "                if len(word) > 0:\n",
    "                    target[word.lower()] += 1\n",
    "        \n",
    "        # remove words common to both classes\n",
    "        for word in set(legit_words.keys()).intersection(set(spam_words.keys())):\n",
    "            del legit_words[word]\n",
    "            del spam_words[word]\n",
    "        \n",
    "        top_legit_words = sorted(legit_words.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        top_spam_words = sorted(spam_words.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        \n",
    "        # store ten times as many words from the spam set\n",
    "        self.legit = set([t[0] for t in top_legit_words[:100]])\n",
    "        self.spam = set([t[0] for t in top_spam_words[:1000]])\n",
    "            \n",
    "    def predict(self, text):\n",
    "        legit_score = 0\n",
    "        spam_score = 0\n",
    "        \n",
    "        for word in re.split(r\"\\W+\", text):\n",
    "            w = word.lower()\n",
    "            if word in self.legit:\n",
    "                legit_score = legit_score + 1\n",
    "            elif word in self.spam:\n",
    "                spam_score = spam_score + 1\n",
    "        \n",
    "        # bias results towards spam in the event of ties\n",
    "        return (legit_score > spam_score) and \"legitimate\" or \"spam\"\n",
    "\n",
    "ssm = SensitiveSpamModel()\n",
    "ssm.fit(unbalanced_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy on our training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.21428571428572"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(ssm, unbalanced_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we've not overfit our training sample, let's check the accuracy on the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.33333333333333"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(ssm, unbalanced_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not quite as good as the results on our training sample, but it's still pretty decent (that is, it's better than just always returning \"spam\" would be given the balance of the classes).  \n",
    "\n",
    "However, we get a different picture if we look at our model's performance on the balanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.5375"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy(ssm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the accuracy is even worse if we look at a sample where the label balance is reversed (i.e., only 10% of documents are spam):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.6"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit_sample = data[data.label == 'legitimate'].sample(900)\n",
    "spam_sample = data[data.label == 'spam'].sample(100)\n",
    "legit_biased = pd.DataFrame.append(legit_sample, spam_sample)\n",
    "model_accuracy(ssm, legit_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to understand the performance of our model with some metric that captures not only the overall accuracy but the accuracy for positive cases and the accuracy for negative cases.  That is, if we assume that our goal is to identify spam documents, we care about:\n",
    "\n",
    "- _true positives_, which are spam documents that our model predicts as spam;\n",
    "- _true negatives_, which are legitimate documents that our model predicts as legitimate;\n",
    "- _false positives_, which are legitimate documents that our model predicts as spam; and\n",
    "- _false negatives_, which are spam documents that our model predicts as legitimate\n",
    "\n",
    "The proportions between these quantities can provide interesting metrics.  For example, the ratio of true positives to actual positives (that is, true positives + false negatives) is called _recall_, which indicates the percentage of spam documents we've selected.  The ratio of true positives to all predicted positives (that is, true positives + false positives) is called _precision_, which indicates the percentage of predicted spam documents that are actually spam.  Ideally, a good classifier would have both high precision and high recall, but in some applications either precision or recall is more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
