{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will process the synthetic Austen/food reviews data and convert it into feature vectors. In later notebooks these feature vectors will be the inputs to models which we will train and eventually use to identify spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/ml-workflows-for-developers/lib/python3.6/site-packages/pyarrow/pandas_compat.py:708: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels = getattr(columns, 'labels', None) or [\n",
      "/anaconda2/envs/ml-workflows-for-developers/lib/python3.6/site-packages/pyarrow/pandas_compat.py:735: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "  return pd.MultiIndex(levels=new_levels, labels=labels, names=columns.names)\n",
      "/anaconda2/envs/ml-workflows-for-developers/lib/python3.6/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data/training.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Indverse Document Frequency (TF-IDF)\n",
    "\n",
    "Term Frequency - Inverse Document Frequency, know as TF-IDF, is a vector of numbers which aim to capture how important different words are within a set of documents. If we consider a standard set of documents (e.g. webpages, news articles, or tweets) one would expect most to contain stop words such as \"a\", \"the\" or \"in\". \n",
    "\n",
    "Considering only raw word count, these words will appear frequently however are not of interest - they don't tell us what the document is about.\n",
    "\n",
    "TF-IDF combines word count, or term frequency, with the inverse document frequency in order to identify words, or terms, which are 'intersting' or important within the document. \n",
    "\n",
    "\n",
    "It is a combination of two different metrics: \n",
    "\n",
    "#### Term Frequency\n",
    "The first is Term Frequency.\n",
    "\n",
    "The term frequency of term $t$ in document $d$, which we denote $Tf(t, d)$, is simply a count of the number of times the term $t$ appears in the document $d$. \n",
    "\n",
    "#### Inverse Document Frequency\n",
    "\n",
    "Inverse Document Frequency, or idf, indicates whether a word is popular or rare, across documents. \n",
    "\n",
    "The inverse document frequency of a term $t$, across a set of $N$ documents is the logarithm of the ratio of $N$ divided by the number of documents in which term $t$ appears: \n",
    "\n",
    "$idf(t) = log\\left(1+ \\frac{N}{\\text{number of documents containing }t +1}\\right)$ +1\n",
    "\n",
    "The $+ 1$ present in the denominator prevents division by zero, which would occur if a term $t$ was present in none of the documents. \n",
    "\n",
    "Multpilying together the TF and the IDF, for a given document $d$ and term $t$ we can compute the TF-IDF:\n",
    "\n",
    "### TF-idf\n",
    "\n",
    "For a term t and a document d, the TF-idf is given by: \n",
    "\n",
    "$tf-idf(t,d) = tf(t,d) \\times idf(t)$\n",
    "\n",
    "\n",
    "\n",
    "The resultant set of vecs for a given document are then normalised. \n",
    "\n",
    "Note: there are variations upon the equations for term frequency and idf, which we do not consider here. The equations given above are those used by the scikit learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: \n",
    "\n",
    "The scikit-learn module contains a set of feature extraction functions which take in text documents and return feature vectors. Tf-idf is computed using the TfidfVectorizer function. By default, the function splits all documents into words, and discounts any 1-letter words as well as punctuation. From there, the function makes a count matrix showing the frequency of the words across the documents. This is then used to compute the Tf-idf.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#initialising the vectorizer, with default parameters. \n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can take a look at the vectorizer. \n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters: \n",
    "    analyzer: {'word', 'char', 'char_wb'} determines whether features (terms) should be words, n-grams (strings of n characters) or only n-grams within word boundaries.\n",
    "    binary: if False, term frequencies are computed as raw counts of terms. if True, term frequencies are taken to be 1 if the term is present at least once, and 0 otherwise.\n",
    "    stop_words: can take in a list of 'stop words' such as 'is', 'was', 'as', and 'the', which will not be included in the terms considered when computing tfidf. note that this can also be controlled by lowering the following parameter: \n",
    "    max_df: takes a float from 0.0 to 1.0. if a term appears in more than max_df fraction of documents it will not be included in the list of considered terms. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x11121 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 129952 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The fit_transform function computes the tf-idf vectors for the simulated data. \n",
    "\n",
    "tf_idf = vectorizer.fit_transform(df[\"text\"])\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '000', '01', '03', '05', '06', '07', '08', '09', '0g']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we look at the first 10 feature names, which we expect to be words because we used the defaults from the , we see that they are nonsense. \n",
    "\n",
    "On one hand, this doesn't really matter - many feature engingeering techniques generate features which are not explainable. On the other hand, we thought we were getting words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (342, 0)\t0.15497173855249344\n",
      "  (935, 0)\t0.09357464860043498\n",
      "  (947, 0)\t0.11922929676832385\n",
      "  (979, 0)\t0.13528214068910932\n",
      "  (1128, 0)\t0.1122534746577168\n",
      "  (1244, 0)\t0.1296948418074572\n",
      "  (1570, 0)\t0.20797516195989615\n",
      "  (1693, 0)\t0.09633887341614138\n",
      "  (1719, 0)\t0.14496356963282298\n",
      "  (1753, 0)\t0.15047467442804296\n",
      "  (1852, 0)\t0.12384227128620147\n",
      "  (1878, 0)\t0.1309043357630492\n",
      "  (1908, 0)\t0.24842177813585903\n",
      "  (1960, 0)\t0.12234175060680996\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf[:,0]) #This tells us that '00' appears in 14 of the docs. let's double check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the correct regular expression to the 'token_pattern' parameter solves this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x10870 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 129258 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf2 = vectorizer2.fit_transform(df[\"text\"])\n",
    "tf_idf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'abate',\n",
       " 'abatement',\n",
       " 'abating',\n",
       " 'abbey',\n",
       " 'aberrant',\n",
       " 'abhor',\n",
       " 'abhorred',\n",
       " 'abhorrence',\n",
       " 'abhorrent']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10830)\t0.037958681483691435\n",
      "  (0, 6342)\t0.06739087879218267\n",
      "  (0, 10770)\t0.11576498332094985\n",
      "  (0, 9839)\t0.07483887828214635\n",
      "  (0, 5989)\t0.10235327957395243\n",
      "  (0, 1466)\t0.09576460459709604\n",
      "  (0, 8733)\t0.16614142192828737\n",
      "  (0, 407)\t0.0724898180628215\n",
      "  (0, 9721)\t0.0562431185808884\n",
      "  (0, 532)\t0.04322500839842098\n",
      "  (0, 6762)\t0.06173256912470132\n",
      "  (0, 1710)\t0.11776731739952018\n",
      "  (0, 10614)\t0.05799973516054904\n",
      "  (0, 355)\t0.12050793329768797\n",
      "  (0, 6552)\t0.06285629186780343\n",
      "  (0, 8375)\t0.10120292684440609\n",
      "  (0, 9707)\t0.06673957630577965\n",
      "  (0, 6449)\t0.06573869626351998\n",
      "  (0, 1194)\t0.13454285150742012\n",
      "  (0, 5220)\t0.1047942917661463\n",
      "  (0, 9727)\t0.03982734423115072\n",
      "  (0, 10653)\t0.05441513078643289\n",
      "  (0, 7761)\t0.09613644805049391\n",
      "  (0, 9852)\t0.09504009808867642\n",
      "  (0, 4610)\t0.16118150841063827\n",
      "  :\t:\n",
      "  (1998, 8747)\t0.20770317666438837\n",
      "  (1999, 9839)\t0.08052063964708878\n",
      "  (1999, 407)\t0.07799323897285004\n",
      "  (1999, 9707)\t0.10770965901554154\n",
      "  (1999, 5220)\t0.08456269920857518\n",
      "  (1999, 867)\t0.17514218469827522\n",
      "  (1999, 5207)\t0.10004219860540935\n",
      "  (1999, 9710)\t0.14721890250015907\n",
      "  (1999, 6527)\t0.11108838338918912\n",
      "  (1999, 6643)\t0.08547995985101539\n",
      "  (1999, 4532)\t0.244436877914194\n",
      "  (1999, 6346)\t0.11754502163855308\n",
      "  (1999, 9880)\t0.1929663042552897\n",
      "  (1999, 5776)\t0.18356362565690754\n",
      "  (1999, 9717)\t0.14596348450506136\n",
      "  (1999, 944)\t0.22605008056850723\n",
      "  (1999, 9997)\t0.23500514737701833\n",
      "  (1999, 9733)\t0.24673327212985327\n",
      "  (1999, 10549)\t0.16590196306857782\n",
      "  (1999, 5373)\t0.33483860620663525\n",
      "  (1999, 9606)\t0.1906459594211959\n",
      "  (1999, 3406)\t0.23869364938134635\n",
      "  (1999, 12)\t0.30340339807268546\n",
      "  (1999, 3297)\t0.3056269000575683\n",
      "  (1999, 9232)\t0.3677612480243218\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
