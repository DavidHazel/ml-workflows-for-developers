{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet(\"data/training.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>You must write to me. Catherine sighed. And th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Who would have thought Mr. Crawford sure of he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>He had only himself to please in his choice: h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Oh! One accompaniment to her song took her agr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>As soon as breakfast was over, she went to her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Mrs Clay's selfishness was not so great as to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>But self, though it would intrude, could not e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Elizabeth, though she did not wish to slight. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Edmund had descended from that moral elevation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>I read up on the morrow the Crawfords were eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>But even this encouragement failed, for he wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>All the better. Catherine's silent appeal to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Got me about 250 feet in a battle.. Their jour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>For ten minutes she could hear of no situation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>She had a little better with milk, but kids or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>These feelings rapidly restored his comfort, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>They found Mr. Bennet still up. That, from suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>They have a odd plastic kinda of taste. He rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>I hope you will think better of their looks to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>The minute they hear the pouch open! So I will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>I will order this item from this company as lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Mr. Dashwood's strains were more solemn. The b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Quite frankly, if you are speaking of music. L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Then passing through the great gates of the lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>I love the taste of the furniture. So far he a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>The Foundation's principal office is located a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>Everybody likes to go their own way--to chuse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>As soon as she possibly could, of his servants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>I'm a big tea drinker but can't find them here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>We've bought some of these and they've turned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>spam</td>\n",
       "      <td>They are moderately sweet and baked-crispy! so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>spam</td>\n",
       "      <td>Please do not hesitate on getting it. It's als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>spam</td>\n",
       "      <td>There is great natural fruity flavor and crunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>spam</td>\n",
       "      <td>I find myself reaching for Tully's House Blend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>spam</td>\n",
       "      <td>I have not yet tried it as of yet I have not f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>spam</td>\n",
       "      <td>Be cautious when reading product descriptions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>spam</td>\n",
       "      <td>Save your money and try something different......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>spam</td>\n",
       "      <td>You don't have to buy them. They'll totally kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>spam</td>\n",
       "      <td>And got a recording. I can purchase this for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>spam</td>\n",
       "      <td>I had a hit when I bring out the catnip! It wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>spam</td>\n",
       "      <td>Living in Australia for a year and 3 boxes.......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>spam</td>\n",
       "      <td>Put contents into a K-cup and they would munch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>spam</td>\n",
       "      <td>Try this combination if you've seen your milk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>spam</td>\n",
       "      <td>And it's 100 percent because she is eating rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>spam</td>\n",
       "      <td>I love how easy it is to burn the drapes and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>spam</td>\n",
       "      <td>All of the mixes I have tried other brands of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>spam</td>\n",
       "      <td>What a difference. Thought they would be offer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>spam</td>\n",
       "      <td>I love the cheddar &amp; can't wait to try some ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>spam</td>\n",
       "      <td>Ummmmm!!! I use these pods I place one pod wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>spam</td>\n",
       "      <td>And, the hardness is just perfect. The high fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>spam</td>\n",
       "      <td>That's it!The end result is that after just th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>spam</td>\n",
       "      <td>He used to have a bit of honey or sweetener of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>spam</td>\n",
       "      <td>I will definitely be re-ordering this tea when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>spam</td>\n",
       "      <td>The roast is lighter than Nabisco, but still w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>spam</td>\n",
       "      <td>I order these 5lb bags all the time, and she l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>spam</td>\n",
       "      <td>Company should buy better materials. So I deci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>spam</td>\n",
       "      <td>I have a French Bulldog that has developed foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>spam</td>\n",
       "      <td>First, I would like other coffee selections to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>spam</td>\n",
       "      <td>We use a lot of coffee, I've tried quite a few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>spam</td>\n",
       "      <td>My kids enjoy them too. LOVE IT. The best thin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                               text\n",
       "0    legitimate  You must write to me. Catherine sighed. And th...\n",
       "1    legitimate  Who would have thought Mr. Crawford sure of he...\n",
       "2    legitimate  He had only himself to please in his choice: h...\n",
       "3    legitimate  Oh! One accompaniment to her song took her agr...\n",
       "4    legitimate  As soon as breakfast was over, she went to her...\n",
       "5    legitimate  Mrs Clay's selfishness was not so great as to ...\n",
       "6    legitimate  But self, though it would intrude, could not e...\n",
       "7    legitimate  Elizabeth, though she did not wish to slight. ...\n",
       "8    legitimate  Edmund had descended from that moral elevation...\n",
       "9    legitimate  I read up on the morrow the Crawfords were eng...\n",
       "10   legitimate  But even this encouragement failed, for he wou...\n",
       "11   legitimate  All the better. Catherine's silent appeal to h...\n",
       "12   legitimate  Got me about 250 feet in a battle.. Their jour...\n",
       "13   legitimate  For ten minutes she could hear of no situation...\n",
       "14   legitimate  She had a little better with milk, but kids or...\n",
       "15   legitimate  These feelings rapidly restored his comfort, w...\n",
       "16   legitimate  They found Mr. Bennet still up. That, from suc...\n",
       "17   legitimate  They have a odd plastic kinda of taste. He rep...\n",
       "18   legitimate  I hope you will think better of their looks to...\n",
       "19   legitimate  The minute they hear the pouch open! So I will...\n",
       "20   legitimate  I will order this item from this company as lo...\n",
       "21   legitimate  Mr. Dashwood's strains were more solemn. The b...\n",
       "22   legitimate  Quite frankly, if you are speaking of music. L...\n",
       "23   legitimate  Then passing through the great gates of the lo...\n",
       "24   legitimate  I love the taste of the furniture. So far he a...\n",
       "25   legitimate  The Foundation's principal office is located a...\n",
       "26   legitimate  Everybody likes to go their own way--to chuse ...\n",
       "27   legitimate  As soon as she possibly could, of his servants...\n",
       "28   legitimate  I'm a big tea drinker but can't find them here...\n",
       "29   legitimate  We've bought some of these and they've turned ...\n",
       "..          ...                                                ...\n",
       "970        spam  They are moderately sweet and baked-crispy! so...\n",
       "971        spam  Please do not hesitate on getting it. It's als...\n",
       "972        spam  There is great natural fruity flavor and crunc...\n",
       "973        spam  I find myself reaching for Tully's House Blend...\n",
       "974        spam  I have not yet tried it as of yet I have not f...\n",
       "975        spam  Be cautious when reading product descriptions ...\n",
       "976        spam  Save your money and try something different......\n",
       "977        spam  You don't have to buy them. They'll totally kn...\n",
       "978        spam  And got a recording. I can purchase this for a...\n",
       "979        spam  I had a hit when I bring out the catnip! It wa...\n",
       "980        spam  Living in Australia for a year and 3 boxes.......\n",
       "981        spam  Put contents into a K-cup and they would munch...\n",
       "982        spam  Try this combination if you've seen your milk ...\n",
       "983        spam  And it's 100 percent because she is eating rea...\n",
       "984        spam  I love how easy it is to burn the drapes and r...\n",
       "985        spam  All of the mixes I have tried other brands of ...\n",
       "986        spam  What a difference. Thought they would be offer...\n",
       "987        spam  I love the cheddar & can't wait to try some ot...\n",
       "988        spam  Ummmmm!!! I use these pods I place one pod wit...\n",
       "989        spam  And, the hardness is just perfect. The high fr...\n",
       "990        spam  That's it!The end result is that after just th...\n",
       "991        spam  He used to have a bit of honey or sweetener of...\n",
       "992        spam  I will definitely be re-ordering this tea when...\n",
       "993        spam  The roast is lighter than Nabisco, but still w...\n",
       "994        spam  I order these 5lb bags all the time, and she l...\n",
       "995        spam  Company should buy better materials. So I deci...\n",
       "996        spam  I have a French Bulldog that has developed foo...\n",
       "997        spam  First, I would like other coffee selections to...\n",
       "998        spam  We use a lot of coffee, I've tried quite a few...\n",
       "999        spam  My kids enjoy them too. LOVE IT. The best thin...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "english = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You must write to me. Catherine sighed. And there are other circumstances which I am now satisfied that I never brewed it. They will read together. Her praise had been given her at different times, but _this_ is the true one. So surrounded, so caressed, she was even positively civil; but it was not directed to me--it was to Mrs. Weston. And besides the operation of a sensible, intelligent man like Mr. Allen. I see that more than a little proud-looking woman of uncordial address, who met her husband's sisters without any affection, and almost without beauty. I walked over the the vending machine so I am very sorry--extremely sorry--But, Miss Smith, indeed!--Oh! Could she but have given Harriet her feelings about it all!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"].get_values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = english(data[\"text\"].get_values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use spaCy to identify parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You is a PRON\n",
      "must is a VERB\n",
      "write is a VERB\n",
      "to is a ADP\n",
      "me is a PRON\n",
      ". is a PUNCT\n",
      "Catherine is a PROPN\n",
      "sighed is a VERB\n",
      ". is a PUNCT\n",
      "And is a CCONJ\n",
      "there is a ADV\n",
      "are is a VERB\n",
      "other is a ADJ\n",
      "circumstances is a NOUN\n",
      "which is a ADJ\n",
      "I is a PRON\n",
      "am is a VERB\n",
      "now is a ADV\n",
      "satisfied is a ADJ\n",
      "that is a ADP\n",
      "I is a PRON\n",
      "never is a ADV\n",
      "brewed is a VERB\n",
      "it is a PRON\n",
      ". is a PUNCT\n",
      "They is a PRON\n",
      "will is a VERB\n",
      "read is a VERB\n",
      "together is a ADV\n",
      ". is a PUNCT\n",
      "Her is a ADJ\n",
      "praise is a NOUN\n",
      "had is a VERB\n",
      "been is a VERB\n",
      "given is a VERB\n",
      "her is a PRON\n",
      "at is a ADP\n",
      "different is a ADJ\n",
      "times is a NOUN\n",
      ", is a PUNCT\n",
      "but is a CCONJ\n",
      "_ is a VERB\n",
      "this is a DET\n",
      "_ is a NOUN\n",
      "is is a VERB\n",
      "the is a DET\n",
      "true is a ADJ\n",
      "one is a NOUN\n",
      ". is a PUNCT\n",
      "So is a ADV\n",
      "surrounded is a VERB\n",
      ", is a PUNCT\n",
      "so is a ADV\n",
      "caressed is a ADJ\n",
      ", is a PUNCT\n",
      "she is a PRON\n",
      "was is a VERB\n",
      "even is a ADV\n",
      "positively is a ADV\n",
      "civil is a ADJ\n",
      "; is a PUNCT\n",
      "but is a CCONJ\n",
      "it is a PRON\n",
      "was is a VERB\n",
      "not is a ADV\n",
      "directed is a VERB\n",
      "to is a ADP\n",
      "me is a PRON\n",
      "-- is a PUNCT\n",
      "it is a PRON\n",
      "was is a VERB\n",
      "to is a ADP\n",
      "Mrs. is a PROPN\n",
      "Weston is a PROPN\n",
      ". is a PUNCT\n",
      "And is a CCONJ\n",
      "besides is a ADP\n",
      "the is a DET\n",
      "operation is a NOUN\n",
      "of is a ADP\n",
      "a is a DET\n",
      "sensible is a ADJ\n",
      ", is a PUNCT\n",
      "intelligent is a ADJ\n",
      "man is a NOUN\n",
      "like is a ADP\n",
      "Mr. is a PROPN\n",
      "Allen is a PROPN\n",
      ". is a PUNCT\n",
      "I is a PRON\n",
      "see is a VERB\n",
      "that is a ADP\n",
      "more is a ADJ\n",
      "than is a ADP\n",
      "a is a DET\n",
      "little is a ADJ\n",
      "proud is a ADJ\n",
      "- is a PUNCT\n",
      "looking is a VERB\n",
      "woman is a NOUN\n",
      "of is a ADP\n",
      "uncordial is a ADJ\n",
      "address is a NOUN\n",
      ", is a PUNCT\n",
      "who is a NOUN\n",
      "met is a VERB\n",
      "her is a ADJ\n",
      "husband is a NOUN\n",
      "'s is a PART\n",
      "sisters is a NOUN\n",
      "without is a ADP\n",
      "any is a DET\n",
      "affection is a NOUN\n",
      ", is a PUNCT\n",
      "and is a CCONJ\n",
      "almost is a ADV\n",
      "without is a ADP\n",
      "beauty is a NOUN\n",
      ". is a PUNCT\n",
      "I is a PRON\n",
      "walked is a VERB\n",
      "over is a ADP\n",
      "the is a DET\n",
      "the is a DET\n",
      "vending is a VERB\n",
      "machine is a NOUN\n",
      "so is a ADP\n",
      "I is a PRON\n",
      "am is a VERB\n",
      "very is a ADV\n",
      "sorry is a ADJ\n",
      "-- is a PUNCT\n",
      "extremely is a ADV\n",
      "sorry is a ADJ\n",
      "-- is a PUNCT\n",
      "But is a CCONJ\n",
      ", is a PUNCT\n",
      "Miss is a PROPN\n",
      "Smith is a PROPN\n",
      ", is a PUNCT\n",
      "indeed!--Oh is a PROPN\n",
      "! is a PUNCT\n",
      "Could is a VERB\n",
      "she is a PRON\n",
      "but is a CCONJ\n",
      "have is a VERB\n",
      "given is a VERB\n",
      "Harriet is a PROPN\n",
      "her is a ADJ\n",
      "feelings is a NOUN\n",
      "about is a ADP\n",
      "it is a PRON\n",
      "all is a DET\n",
      "! is a PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(\"%s is a %s\" % (token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use spaCy to identify the base forms of words -- it does this with a combination of part-of-speech-specific rules and a dictionary of exceptions.  The spaCy component that does this is called a [_lemmatizer_](https://en.wikipedia.org/wiki/Lemma_%28morphology%29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You has a base form of ['you']\n",
      "must has a base form of ['must']\n",
      "write has a base form of ['write']\n",
      "to has a base form of ['to']\n",
      "me has a base form of ['me']\n",
      ". has a base form of ['.']\n",
      "Catherine has a base form of ['catherine']\n",
      "sighed has a base form of ['sigh']\n",
      ". has a base form of ['.']\n",
      "And has a base form of ['and']\n",
      "there has a base form of ['there']\n",
      "are has a base form of ['be']\n",
      "other has a base form of ['othe', 'oth']\n",
      "circumstances has a base form of ['circumstance']\n",
      "which has a base form of ['which']\n",
      "I has a base form of ['i']\n",
      "am has a base form of ['be']\n",
      "now has a base form of ['now']\n",
      "satisfied has a base form of ['satisfied']\n",
      "that has a base form of ['that']\n",
      "I has a base form of ['i']\n",
      "never has a base form of ['never']\n",
      "brewed has a base form of ['brew']\n",
      "it has a base form of ['it']\n",
      ". has a base form of ['.']\n",
      "They has a base form of ['they']\n",
      "will has a base form of ['will']\n",
      "read has a base form of ['read']\n",
      "together has a base form of ['together']\n",
      ". has a base form of ['.']\n",
      "Her has a base form of ['h', 'he']\n",
      "praise has a base form of ['praise']\n",
      "had has a base form of ['have']\n",
      "been has a base form of ['be']\n",
      "given has a base form of ['give']\n",
      "her has a base form of ['her']\n",
      "at has a base form of ['at']\n",
      "different has a base form of ['different']\n",
      "times has a base form of ['time']\n",
      ", has a base form of [',']\n",
      "but has a base form of ['but']\n",
      "_ has a base form of ['_']\n",
      "this has a base form of ['this']\n",
      "_ has a base form of ['_']\n",
      "is has a base form of ['be']\n",
      "the has a base form of ['the']\n",
      "true has a base form of ['true']\n",
      "one has a base form of ['one']\n",
      ". has a base form of ['.']\n",
      "So has a base form of ['so']\n",
      "surrounded has a base form of ['surround']\n",
      ", has a base form of [',']\n",
      "so has a base form of ['so']\n",
      "caressed has a base form of ['caressed']\n",
      ", has a base form of [',']\n",
      "she has a base form of ['she']\n",
      "was has a base form of ['be']\n",
      "even has a base form of ['even']\n",
      "positively has a base form of ['positively']\n",
      "civil has a base form of ['civil']\n",
      "; has a base form of [';']\n",
      "but has a base form of ['but']\n",
      "it has a base form of ['it']\n",
      "was has a base form of ['be']\n",
      "not has a base form of ['not']\n",
      "directed has a base form of ['direct']\n",
      "to has a base form of ['to']\n",
      "me has a base form of ['me']\n",
      "-- has a base form of ['--']\n",
      "it has a base form of ['it']\n",
      "was has a base form of ['be']\n",
      "to has a base form of ['to']\n",
      "Mrs. has a base form of ['mrs.']\n",
      "Weston has a base form of ['weston']\n",
      ". has a base form of ['.']\n",
      "And has a base form of ['and']\n",
      "besides has a base form of ['besides']\n",
      "the has a base form of ['the']\n",
      "operation has a base form of ['operation']\n",
      "of has a base form of ['of']\n",
      "a has a base form of ['a']\n",
      "sensible has a base form of ['sensible']\n",
      ", has a base form of [',']\n",
      "intelligent has a base form of ['intelligent']\n",
      "man has a base form of ['man']\n",
      "like has a base form of ['like']\n",
      "Mr. has a base form of ['mr.']\n",
      "Allen has a base form of ['allen']\n",
      ". has a base form of ['.']\n",
      "I has a base form of ['i']\n",
      "see has a base form of ['see']\n",
      "that has a base form of ['that']\n",
      "more has a base form of ['more']\n",
      "than has a base form of ['than']\n",
      "a has a base form of ['a']\n",
      "little has a base form of ['little']\n",
      "proud has a base form of ['proud']\n",
      "- has a base form of ['-']\n",
      "looking has a base form of ['look']\n",
      "woman has a base form of ['woman']\n",
      "of has a base form of ['of']\n",
      "uncordial has a base form of ['uncordial']\n",
      "address has a base form of ['addres']\n",
      ", has a base form of [',']\n",
      "who has a base form of ['who']\n",
      "met has a base form of ['meet']\n",
      "her has a base form of ['h', 'he']\n",
      "husband has a base form of ['husband']\n",
      "'s has a base form of [\"'s\"]\n",
      "sisters has a base form of ['sister']\n",
      "without has a base form of ['without']\n",
      "any has a base form of ['any']\n",
      "affection has a base form of ['affection']\n",
      ", has a base form of [',']\n",
      "and has a base form of ['and']\n",
      "almost has a base form of ['almost']\n",
      "without has a base form of ['without']\n",
      "beauty has a base form of ['beauty']\n",
      ". has a base form of ['.']\n",
      "I has a base form of ['i']\n",
      "walked has a base form of ['walk']\n",
      "over has a base form of ['over']\n",
      "the has a base form of ['the']\n",
      "the has a base form of ['the']\n",
      "vending has a base form of ['vend']\n",
      "machine has a base form of ['machine']\n",
      "so has a base form of ['so']\n",
      "I has a base form of ['i']\n",
      "am has a base form of ['be']\n",
      "very has a base form of ['very']\n",
      "sorry has a base form of ['sorry']\n",
      "-- has a base form of ['--']\n",
      "extremely has a base form of ['extremely']\n",
      "sorry has a base form of ['sorry']\n",
      "-- has a base form of ['--']\n",
      "But has a base form of ['but']\n",
      ", has a base form of [',']\n",
      "Miss has a base form of ['miss']\n",
      "Smith has a base form of ['smith']\n",
      ", has a base form of [',']\n",
      "indeed!--Oh has a base form of ['indeed!--oh']\n",
      "! has a base form of ['!']\n",
      "Could has a base form of ['could']\n",
      "she has a base form of ['she']\n",
      "but has a base form of ['but']\n",
      "have has a base form of ['have']\n",
      "given has a base form of ['give']\n",
      "Harriet has a base form of ['harriet']\n",
      "her has a base form of ['h', 'he']\n",
      "feelings has a base form of ['feeling']\n",
      "about has a base form of ['about']\n",
      "it has a base form of ['it']\n",
      "all has a base form of ['all']\n",
      "! has a base form of ['!']\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(\"%s has a base form of %s\" % (token.text, lemmatizer(token.text, token.pos_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this process to the entire data frame if we'd like, but it might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmas(s):\n",
    "    return \" \".join([lemmatizer(token.text, token.pos_)[0] for token in english(s) if str(token.text) not in [\".\", \"!\", \"?\", \",\" \";\"]])\n",
    "\n",
    "data[\"lemmas\"] = data[\"text\"].apply(lemmas,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having words in more-or-less canonical forms is useful, but we'll want a different representation to identify structure in our data.  Remember, our goal is to be able to learn a function that can separate between documents that are likely to represent legitimate messages (i.e., prose in the style of Jane Austen) or spam messages (i.e., prose in the style of food-product reviews).  \n",
    "\n",
    "_Feature engineering_ is the name for the process of turning real-world data into a form that a machine learning algorithm can take advantage of.  You'll learn more about this process in the next notebook; here, we'll just take a very basic approach that will let us visualize our data.  Logically, here's what we'll do:\n",
    "\n",
    "1.  We'll collect word counts for each example, showing us how frequent each word is in a given document;\n",
    "2.  We'll then turn those raw counts into frequencies (i.e., for a given word what percentage of words in given document are that word?), giving us a mapping from words to frequencies for each document;\n",
    "3.  Finally, we'll encode these mappings as fixed-size vectors in a space-efficient way, by using a hash function to determine which vector element should get a given frequency.  Hashing has a few advantages, but for our purposes the most important advantage is that we don't need to know all of the words we might see in advance. \n",
    "\n",
    "(That's what we'll _logically_ do -- we'll _actually_ do these steps a bit out of order because it will make our code simpler and more efficient without changing the results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hashing_frequency(vecsize, h):\n",
    "    \"\"\" \n",
    "    returns a function that will collect term frequencies \n",
    "    into a vector with _vecsize_ elements and will use \n",
    "    the hash function _h_ to choose which vector element \n",
    "    to update for a given term\n",
    "    \"\"\"\n",
    "    \n",
    "    def hf(words):\n",
    "        if type(words) is type(\"\"):\n",
    "            # handle both lists of words and space-delimited strings\n",
    "            words = words.split(\" \")\n",
    "            \n",
    "        result = np.zeros(vecsize)\n",
    "        for term in words:\n",
    "            result[h(term) % vecsize] += 1.0\n",
    "        result = result / sum(result)\n",
    "        return result\n",
    "    \n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"vectors\"] = data[\"lemmas\"].apply(hashing_frequency(2048, hash), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now instead of having documents (which we had from the raw data) or lists of word lemmas, we have vectors representing word lemma frequencies.  Because we've hashed lemmas into these vectors, we can't in general reconstruct the list of words from a vector, but we _do_ know that if the same lemma appears in two documents, their vectors will reflect it in corresponding buckets.\n",
    "\n",
    "However, we've generated a 2,048-element vector.  Recall that our ultimate goal is to place documents in space so that we can identify a way to separate legitimate documents from spam documents.  Our 2,048-element vector is a point in a space, but it's a point in a space that most of our geometric intuitions don't apply to (some of us have enough trouble navigating the three dimensions of the physical world).  Let's use a very basic technique to project these vectors to a much smaller space that we can visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import random_projection\n",
    "DIMENSIONS = 2\n",
    "\n",
    "rp = random_projection.SparseRandomProjection(DIMENSIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
